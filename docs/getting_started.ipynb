{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "This is a minimal guide on how to get started using SEB. If you feel like the documentation is lacking feel free to file an [issue](https://github.com/KennethEnevoldsen/scandinavian-embedding-benchmark/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CLI\n",
    "\n",
    "SEB comes with a simple cli to allow you to run models. This section will show a minimal example of how to use the CLI but if you want to know more check out the CLI documentation. To get a list of available commands you can simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available commands:\n",
      "\n",
      "  run   Runs the Benchmark on a specified model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "seb --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or for more on the specific command you can call `seb {command} --help`. To run a model using the CLI you can run it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:seb.cli.run:Model registered in SEB. Loading from registry.\n",
      "\n",
      "Running all-MiniLM-L6-v2:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on Angry Tweets:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on LCC:   0%|          | 0/14 [00:00<?, ?it/s]         \u001b[A\n",
      "Running all-MiniLM-L6-v2 on Bornholm Parallel:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on DKHate:   0%|          | 0/14 [00:00<?, ?it/s]           \u001b[A\n",
      "Running all-MiniLM-L6-v2 on Da Political Comments:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on Massive Intent:   0%|          | 0/14 [00:00<?, ?it/s]       \u001b[A\n",
      "Running all-MiniLM-L6-v2 on Massive Scenario:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on ScaLA:   0%|          | 0/14 [00:00<?, ?it/s]           \u001b[A\n",
      "Running all-MiniLM-L6-v2 on Language Identification:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on NoReC:   0%|          | 0/14 [00:00<?, ?it/s]                  \u001b[A\n",
      "Running all-MiniLM-L6-v2 on Norwegian parliament:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Running all-MiniLM-L6-v2 on SweReC:   0%|          | 0/14 [00:00<?, ?it/s]              \u001b[A\n",
      "Running all-MiniLM-L6-v2 on DaLAJ:   0%|          | 0/14 [00:00<?, ?it/s] \u001b[A\n",
      "Running all-MiniLM-L6-v2 on SweFAQ:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                          \u001b[A"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "seb run sentence-transformers/all-MiniLM-L6-v2 --output-path model_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a task\n",
    "To run a task you will need to fetch the task amd a model run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import seb\n",
    "\n",
    "model = seb.get_model(\"jonfd/electra-small-nordic\")\n",
    "task = seb.get_task(\"DKHate\")\n",
    "\n",
    "# initialize benchmark with tasks\n",
    "benchmark = seb.Benchmark(tasks=[task])\n",
    "\n",
    "# benchmark the model\n",
    "benchmark_result = benchmark.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkResults(meta=ModelMeta(name='electra-small-nordic', description=None, huggingface_name='jonfd/electra-small-nordic', reference='https://huggingface.co/jonfd/electra-small-nordic', languages=['da', 'nb', 'sv', 'nn'], open_source=True, embedding_size=256), task_results=[TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 30, 13, 55, 38, 480327), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.8950480900418238, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.013877821318913264, 'main_score': 0.5945288753799393}}, main_score='accuracy')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_result  # examine output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 30, 13, 55, 38, 480327), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.8950480900418238, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.013877821318913264, 'main_score': 0.5945288753799393}}, main_score='accuracy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_result[0]  # examine the results for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the Benchmark\n",
    "Reproducing the benchmark is easy and is doable simply using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running text-embedding-ada-002: 100%|██████████| 30/30 [00:00<00:00, 31.41it/s]                       \n",
      "Running text-embedding-ada-002: 100%|██████████| 30/30 [00:00<00:00, 40.98it/s]                       \n",
      "Running text-embedding-ada-002: 100%|██████████| 30/30 [00:00<00:00, 34.49it/s]                       \n",
      "Running text-embedding-ada-002: 100%|██████████| 30/30 [00:00<00:00, 33.02it/s]                       \n"
     ]
    }
   ],
   "source": [
    "results = seb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the full benchmark on all the registrered models as well as all the registrered datasets. Note that all benchmark results are cached as included as a part of the package, this means that you won't have to rerun results that are already run.\n",
    "\n",
    "The results are returned as a dictionary of where the keys represent the benchmark and values are a list of benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Mainland Scandinavian', 'Danish', 'Norwegian', 'Swedish'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkResults(meta=ModelMeta(name='embed-multilingual-v3.0', description=None, huggingface_name=None, reference='https://huggingface.co/Cohere/Cohere-embed-multilingual-v3.0', languages=[], open_source=False, embedding_size=1024), task_results=[TaskResult(task_name='Angry Tweets', task_description='A sentiment dataset with 3 classes (positiv, negativ, neutral) for Danish tweets', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 49, 12, 771515), scores={'da': {'accuracy': 0.589111747851003, 'f1': 0.5800442049443755, 'accuracy_stderr': 0.02208679883291171, 'f1_stderr': 0.0205122012161316, 'main_score': 0.589111747851003}}, main_score='accuracy'), TaskResult(task_name='LCC', task_description='The leipzig corpora collection, annotated for sentiment', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 49, 26, 932464), scores={'da': {'accuracy': 0.604, 'f1': 0.6045645057913338, 'accuracy_stderr': 0.034794635601866374, 'f1_stderr': 0.03469599049990604, 'main_score': 0.604}}, main_score='accuracy'), TaskResult(task_name='Bornholm Parallel', task_description='Danish Bornholmsk Parallel Corpus. Bornholmsk is a Danish dialect spoken on the island of Bornholm, Denmark. Historically it is a part of east Danish which was also spoken in Scania and Halland, Sweden.', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 49, 36, 527514), scores={'da': {'precision': 0.32936800976800973, 'recall': 0.44, 'f1': 0.35662395382395384, 'accuracy': 0.44, 'main_score': 0.35662395382395384}, 'da-bornholm': {'precision': 0.32936800976800973, 'recall': 0.44, 'f1': 0.35662395382395384, 'accuracy': 0.44, 'main_score': 0.35662395382395384}}, main_score='f1'), TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 49, 50, 255921), scores={'da': {'accuracy': 0.6878419452887539, 'f1': 0.5754292687298207, 'ap': 0.20997905156829208, 'accuracy_stderr': 0.06866946902787877, 'f1_stderr': 0.05597856609791259, 'ap_stderr': 0.041374474998038195, 'main_score': 0.6878419452887539}}, main_score='accuracy'), TaskResult(task_name='Da Political Comments', task_description='A dataset of Danish political comments rated for sentiment', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 50, 10, 324853), scores={'da': {'accuracy': 0.4340732519422864, 'f1': 0.4053270285804955, 'accuracy_stderr': 0.032140309062267135, 'f1_stderr': 0.023638097975834493, 'main_score': 0.4340732519422864}}, main_score='accuracy'), TaskResult(task_name='Massive Intent', task_description='MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 53, 9, 380602), scores={'da': {'accuracy': 0.6654673839946199, 'f1': 0.637776757848086, 'accuracy_stderr': 0.013775850305668075, 'f1_stderr': 0.013697069516200004, 'main_score': 0.6654673839946199}, 'nb': {'accuracy': 0.6719233355749832, 'f1': 0.6446330966343174, 'accuracy_stderr': 0.014987021986641317, 'f1_stderr': 0.015961831197456297, 'main_score': 0.6719233355749832}, 'sv': {'accuracy': 0.6960659045057163, 'f1': 0.6766171113444953, 'accuracy_stderr': 0.018051479607875766, 'f1_stderr': 0.015799973996323373, 'main_score': 0.6960659045057163}}, main_score='accuracy'), TaskResult(task_name='Massive Scenario', task_description='MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 55, 22, 745539), scores={'da': {'accuracy': 0.7438466711499665, 'f1': 0.7366895282457374, 'accuracy_stderr': 0.009173827073322146, 'f1_stderr': 0.009689167533545422, 'main_score': 0.7438466711499665}, 'nb': {'accuracy': 0.7384330867518494, 'f1': 0.7322937369822327, 'accuracy_stderr': 0.010778239699265587, 'f1_stderr': 0.009692505530198426, 'main_score': 0.7384330867518494}, 'sv': {'accuracy': 0.7574310692669805, 'f1': 0.7517381645229646, 'accuracy_stderr': 0.012045171284048639, 'f1_stderr': 0.01166614751961022, 'main_score': 0.7574310692669805}}, main_score='accuracy'), TaskResult(task_name='ScaLA', task_description='A linguistic acceptability task for Danish, Norwegian Bokmål Norwegian Nynorsk and Swedish.', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 56, 53, 574324), scores={'da': {'accuracy': 0.508349609375, 'f1': 0.5056089560837547, 'ap': 0.504270609280779, 'accuracy_stderr': 0.005552039093165773, 'f1_stderr': 0.0061853850329536605, 'ap_stderr': 0.002852190710808234, 'main_score': 0.508349609375}, 'nb': {'accuracy': 0.5068359375, 'f1': 0.5028082737136484, 'ap': 0.5035329742210839, 'accuracy_stderr': 0.007542325435410011, 'f1_stderr': 0.00918042108637033, 'ap_stderr': 0.003932270904497431, 'main_score': 0.5068359375}, 'sv': {'accuracy': 0.5068359375, 'f1': 0.5043076304129289, 'ap': 0.5035176962385777, 'accuracy_stderr': 0.007069125418302288, 'f1_stderr': 0.008946547886637759, 'ap_stderr': 0.003617227768456735, 'main_score': 0.5068359375}, 'nn': {'accuracy': 0.5056640625, 'f1': 0.5036120708948217, 'ap': 0.5029426709291045, 'accuracy_stderr': 0.00923148617038787, 'f1_stderr': 0.008517687177028652, 'ap_stderr': 0.004577040920590477, 'main_score': 0.5056640625}}, main_score='accuracy'), TaskResult(task_name='Language Identification', task_description='A dataset for Nordic language identification.', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 57, 45, 658096), scores={'da': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}, 'sv': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}, 'nb': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}, 'nn': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}, 'is': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}, 'fo': {'accuracy': 0.7872333333333332, 'f1': 0.7806507474390829, 'accuracy_stderr': 0.0072419303749453184, 'f1_stderr': 0.007780108614777823, 'main_score': 0.7872333333333332}}, main_score='accuracy'), TaskResult(task_name='NoReC', task_description='A Norwegian dataset for sentiment classification on review', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 58, 5, 614937), scores={'nb': {'accuracy': 0.66064453125, 'f1': 0.6496479700114184, 'accuracy_stderr': 0.016785799855301105, 'f1_stderr': 0.015430913489577222, 'main_score': 0.66064453125}}, main_score='accuracy'), TaskResult(task_name='Norwegian parliament', task_description='Norwegian parliament speeches annotated for sentiment', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 58, 29, 889565), scores={'nb': {'accuracy': 0.5998333333333333, 'f1': 0.5973067990191325, 'ap': 0.5608034931676839, 'accuracy_stderr': 0.027610384519838403, 'f1_stderr': 0.028539401198880567, 'ap_stderr': 0.019753617006441477, 'main_score': 0.5998333333333333}}, main_score='accuracy'), TaskResult(task_name='SweReC', task_description='A Swedish dataset for sentiment classification on review', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 58, 54, 160842), scores={'sv': {'accuracy': 0.841796875, 'f1': 0.7577270299740745, 'accuracy_stderr': 0.009037820821425667, 'f1_stderr': 0.006868062554294348, 'main_score': 0.841796875}}, main_score='accuracy'), TaskResult(task_name='DaLAJ', task_description='A Swedish dataset for linguistic acceptability. Available as a part of Superlim.', task_version='1.1.1.dev0', time_of_run=datetime.datetime(2023, 11, 15, 15, 59, 10, 156497), scores={'sv': {'accuracy': 0.4996621621621622, 'f1': 0.4973466706921844, 'ap': 0.499855758478619, 'accuracy_stderr': 0.004779075489622282, 'f1_stderr': 0.0062521755097949515, 'ap_stderr': 0.0023683552238971985, 'main_score': 0.4996621621621622}}, main_score='accuracy'), TaskResult(task_name='SweFAQ', task_description='A Swedish QA dataset derived from FAQ', task_version='0.0.1', time_of_run=datetime.datetime(2023, 11, 15, 15, 59, 20, 656254), scores={'sv': {'ndcg_at_1': 0.5809, 'ndcg_at_3': 0.71038, 'ndcg_at_5': 0.7374, 'ndcg_at_10': 0.75234, 'ndcg_at_100': 0.76935, 'ndcg_at_1000': 0.77073, 'map_at_1': 0.5809, 'map_at_3': 0.67836, 'map_at_5': 0.69366, 'map_at_10': 0.69972, 'map_at_100': 0.70353, 'map_at_1000': 0.70361, 'recall_at_1': 0.5809, 'recall_at_3': 0.80312, 'recall_at_5': 0.86745, 'recall_at_10': 0.91423, 'recall_at_100': 0.99025, 'recall_at_1000': 1.0, 'precision_at_1': 0.5809, 'precision_at_3': 0.26771, 'precision_at_5': 0.17349, 'precision_at_10': 0.09142, 'precision_at_100': 0.0099, 'precision_at_1000': 0.001, 'mrr_at_1': 0.5809, 'mrr_at_3': 0.67836, 'mrr_at_5': 0.69366, 'mrr_at_10': 0.69972, 'mrr_at_100': 0.70353, 'mrr_at_1000': 0.70361}}, main_score='ndcg_at_10')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results.keys())\n",
    "\n",
    "results[\"Danish\"][0]  # result for the first model in the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a model\n",
    "\n",
    "The benchmark uses a registry to add models. A model in `seb` includes two thing. 1) a metadata object (`seb.ModelMeta`) describing the metadata of the model and 2) a loader for the model itself, which is an object that needs an encode methods as described by the `seb.ModelInterface`. Here is a minimal example of how to add a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import seb\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "def get_my_model() -> SentenceTransformer:\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "@seb.models.register(model_name)  # add the model to the registry\n",
    "def create_all_mini_lm_l6_v2() -> seb.EmbeddingModel:\n",
    "    hf_name = model_name\n",
    "\n",
    "    # create meta data\n",
    "    meta = seb.ModelMeta(\n",
    "        name=hf_name.split(\"/\")[-1],\n",
    "        huggingface_name=hf_name,\n",
    "        reference=\"https://huggingface.co/{hf_name}\",\n",
    "        languages=[],\n",
    "        embedding_size=384,\n",
    "    )\n",
    "    return seb.EmbeddingModel(\n",
    "        loader=get_my_model,\n",
    "        meta=meta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to use the CLI with one of your own added models you can import registrered functions from a file specified using the `--code` flag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mteb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
