{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "This is a minimal documentation at the moment at I am unsure how many will use the package. If you do want to use the package, but feel like the documentation is lacking feel free to open an issue on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a task\n",
    "To run a task you will need to fetch the task, a model run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seb\n",
    "\n",
    "tasks = [\"DKHate\"]\n",
    "model = seb.get_model(\"jonfd/electra-small-nordic\")\n",
    "\n",
    "# initialize benchmark with tasks\n",
    "benchmark = seb.Benchmark(tasks=tasks)\n",
    "\n",
    "# benchmark the model\n",
    "benchmark_result = benchmark.evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkResults(meta=ModelMeta(name='electra-small-nordic', description=None, huggingface_name='jonfd/electra-small-nordic', reference='https://huggingface.co/{hf_name}', languages=['da', 'no', 'sv']), task_results=[TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_result  # examine output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_result[0]  # examine the results for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a model\n",
    "\n",
    "The benchmark uses a registry to add models. A model in `seb` includes two thing. 1) a metadata object (`seb.ModelMeta`) describing the metadata of the model and 2) a loader for the model itself, which is an object that needs an encode methods as described by the `seb.ModelInterface`. Here is an example of how to add a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seb\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "def get_my_model():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "@seb.models.register(model_name)\n",
    "def create_all_mini_lm_l6_v2() -> seb.SebModel:\n",
    "    hf_name = model_name\n",
    "\n",
    "    # create meta data\n",
    "    meta = seb.ModelMeta(\n",
    "        name=hf_name.split(\"/\")[-1],\n",
    "        huggingface_name=hf_name,\n",
    "        reference=\"https://huggingface.co/{hf_name}\",\n",
    "        languages=[],\n",
    "    )\n",
    "    return seb.SebModel(\n",
    "        loader=get_my_model,\n",
    "        meta=meta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the Benchmark\n",
    "Reproducing the benchmark is easy and is doable simply using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deliberately not running this test as it takes a while\n",
    "from seb import run_benchmark\n",
    "\n",
    "results = run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the full benchmark on all the registrered models as well as all the registrered datasets. The results are returned as a dictionary of where the keys represent the benchmark and values are a list of benchmark results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mteb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
