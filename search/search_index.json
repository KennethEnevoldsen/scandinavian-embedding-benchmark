{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scandinavian Embedding Benchmark","text":"<p>This is the documentation for the Scandinavian Embedding Benchmark. This benchmark is intended to evaluate the sentence/document embeddings of language models for mainland Scandinavian Languages.</p> <p>Intended uses for this benchmark:</p> <ul> <li>Evaluating document embeddings of Scandinavian language models</li> <li>Evaluating document embeddings of multilingual models for Scandinavian languages</li> <li>Allow ranking of competing Scandinavian and multilingual models using no more compute than what a consumer laptop can provide </li> </ul> AllDanishNorwegianSwedish <p> </p> <p></p> <p></p> <p></p>"},{"location":"#comparison-to-other-benchmarks","title":"Comparison to other benchmarks","text":"<p>If you use this benchmark for a relative ranking of language models where you plan to fine-tune the models I would recommend looking at ScandEval, which benchmarks the model using a cross-validated fine-tuning. It also includes structured prediction tasks such as named entity recognition. Many of the tasks in this embedding benchmark are also included in ScandEval, and an attempt has been made to use the same versions. A few tasks (ScandiQA) are included in ScandEval, but not in this benchmark as they are human translations of an English dataset.</p> <p>The tasks within this benchmark are also included in the MTEB leaderboard, though the aggregation methods are slightly different. MTEB is primarily an English embedding benchmark, with a few multilingual tasks and additional languages. The tasks were also added to the MTEB leaderboard as a part of this project.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#general","title":"General","text":"<p>General function for dealing with tasks and models implemented in SEB.</p>"},{"location":"api/#seb.registries.get_task","title":"<code>seb.registries.get_task(name)</code>","text":"<p>Fetches a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the task.</p> required <p>Returns:</p> Type Description <code>Task</code> <p>A task.</p> Source code in <code>seb/registries.py</code> <pre><code>def get_task(name: str) -&gt; Task:\n\"\"\"\n    Fetches a task by name.\n\n    Args:\n        name: The name of the task.\n\n    Returns:\n        A task.\n    \"\"\"\n    return tasks.get(name)()\n</code></pre>"},{"location":"api/#seb.registries.get_all_tasks","title":"<code>seb.registries.get_all_tasks()</code>","text":"<p>Returns all tasks implemented in SEB.</p> <p>Returns:</p> Type Description <code>list[seb.interfaces.task.Task]</code> <p>A list of all tasks in SEB.</p> Source code in <code>seb/registries.py</code> <pre><code>def get_all_tasks() -&gt; list[Task]:\n\"\"\"\n    Returns all tasks implemented in SEB.\n\n    Returns:\n        A list of all tasks in SEB.\n    \"\"\"\n    return [get_task(task_name) for task_name in tasks.get_all()]\n</code></pre>"},{"location":"api/#seb.registries.get_model","title":"<code>seb.registries.get_model(name)</code>","text":"<p>Fetches a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model.</p> required <p>Returns:</p> Type Description <code>SebModel</code> <p>A model including metadata.</p> Source code in <code>seb/registries.py</code> <pre><code>def get_model(name: str) -&gt; SebModel:\n\"\"\"\n    Fetches a model by name.\n\n    Args:\n        name: The name of the model.\n\n    Returns:\n        A model including metadata.\n    \"\"\"\n    return models.get(name)()\n</code></pre>"},{"location":"api/#seb.registries.get_all_models","title":"<code>seb.registries.get_all_models()</code>","text":"<p>Get all the models implemented in SEB.</p> <p>Returns:</p> Type Description <code>list[seb.interfaces.model.SebModel]</code> <p>A list of all models in SEB.</p> Source code in <code>seb/registries.py</code> <pre><code>def get_all_models() -&gt; list[SebModel]:\n\"\"\"\n    Get all the models implemented in SEB.\n\n    Returns:\n        A list of all models in SEB.\n    \"\"\"\n    return [get_model(model_name) for model_name in models.get_all()]\n</code></pre>"},{"location":"api/#benchmark","title":"Benchmark","text":""},{"location":"api/#seb.benchmark.Benchmark","title":"<code> seb.benchmark.Benchmark        </code>","text":"<p>Benchmark is the main orchestrator of the SEB benchmark.</p> Source code in <code>seb/benchmark.py</code> <pre><code>class Benchmark:\n\"\"\"\n    Benchmark is the main orchestrator of the SEB benchmark.\n    \"\"\"\n\n    def __init__(\n        self,\n        languages: Optional[list[str]] = None,\n        tasks: Optional[Union[Iterable[str], Iterable[Task]]] = None,\n    ) -&gt; None:\n\"\"\"\n        Initialize the benchmark.\n\n        Args:\n            languages: A list of languages to run the benchmark on. If None, all languages are used.\n            tasks: The tasks to run the benchmark on. If None, all tasks are used. Can either be specified as strings or as Task objects.\n        \"\"\"\n        self.languages = languages\n\n        self.tasks = self.get_tasks(tasks, languages)\n        self.task_names = [task.name for task in self.tasks]\n\n    @staticmethod\n    def get_tasks(\n        tasks: Optional[Union[Iterable[str], Iterable[Task]]],\n        languages: Optional[list[str]],\n    ) -&gt; list[Task]:\n\"\"\"\n        Get the tasks for the benchmark.\n\n        Returns:\n            A list of tasks.\n        \"\"\"\n        _tasks = []\n\n        if tasks is None:\n            _tasks = get_all_tasks()\n        else:\n            for task in tasks:\n                if isinstance(task, str):\n                    _tasks.append(get_task(task))\n                elif isinstance(task, Task):\n                    _tasks.append(task)\n                else:\n                    raise ValueError(f\"Invalid task type: {type(task)}\")\n\n        if languages is not None:\n            langs = set(languages)\n            _tasks = [task for task in _tasks if set(task.languages) &amp; langs]\n\n        return _tasks\n\n    def evaluate_model(\n        self,\n        model: SebModel,\n        use_cache: bool = True,\n        run_model: bool = True,\n        raise_errors: bool = True,\n        cache_dir: Optional[Path] = None,\n        verbose: bool = True,\n    ) -&gt; BenchmarkResults:\n\"\"\"\n        Evaluate a model on the benchmark.\n\n        Args:\n            model: The model to evaluate.\n            use_cache: Whether to use the cache.\n            run_model: Whether to run the model if the cache is not present.\n            raise_errors: Whether to raise errors.\n            cache_dir: The cache directory to use. If None, the default cache directory is used.\n            verbose: Whether to show a progress bar.\n\n        Returns:\n            The results of the benchmark.\n        \"\"\"\n        task_results = []\n        pbar = tqdm(\n            self.tasks,\n            position=1,\n            desc=f\"Running {model.meta.name}\",\n            leave=False,\n            disable=not verbose,\n        )\n        for task in pbar:\n            pbar.set_description(f\"Running {model.meta.name} on {task.name}\")\n            task_result = run_task(\n                task,\n                model,\n                use_cache=use_cache,\n                run_model=run_model,\n                raise_errors=raise_errors,\n                cache_dir=cache_dir,\n            )\n            task_results.append(task_result)\n\n        return BenchmarkResults(meta=model.meta, task_results=task_results)\n\n    def evaluate_models(\n        self,\n        models: list[SebModel],\n        use_cache: bool = True,\n        run_model: bool = True,\n        raise_errors: bool = True,\n        cache_dir: Optional[Path] = None,\n        verbose: bool = True,\n    ) -&gt; list[BenchmarkResults]:\n\"\"\"\n        Evaluate a list of models on the benchmark.\n\n        Args:\n            models: The models to evaluate.\n            use_cache: Whether to use the cache.\n            run_model: Whether to run the model if the cache is not present.\n            raise_errors: Whether to raise errors.\n            cache_dir: The cache directory to use. If None, the default cache directory is used.\n            verbose: Whether to show a progress bar.\n\n        Returns:\n            The results of the benchmark, once for each model.\n        \"\"\"\n        results = []\n        pbar = tqdm(\n            models,\n            position=0,\n            desc=\"Running Benchmark\",\n            leave=True,\n            disable=not verbose,\n        )\n\n        for model in pbar:\n            pbar.set_description(f\"Running {model.meta.name}\")\n            results.append(\n                self.evaluate_model(\n                    model,\n                    use_cache=use_cache,\n                    run_model=run_model,\n                    raise_errors=raise_errors,\n                    cache_dir=cache_dir,\n                    verbose=verbose,\n                ),\n            )\n        return results\n</code></pre>"},{"location":"api/#seb.benchmark.Benchmark.__init__","title":"<code>__init__(self, languages=None, tasks=None)</code>  <code>special</code>","text":"<p>Initialize the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Optional[list[str]]</code> <p>A list of languages to run the benchmark on. If None, all languages are used.</p> <code>None</code> <code>tasks</code> <code>Union[collections.abc.Iterable[str], collections.abc.Iterable[seb.interfaces.task.Task]]</code> <p>The tasks to run the benchmark on. If None, all tasks are used. Can either be specified as strings or as Task objects.</p> <code>None</code> Source code in <code>seb/benchmark.py</code> <pre><code>def __init__(\n    self,\n    languages: Optional[list[str]] = None,\n    tasks: Optional[Union[Iterable[str], Iterable[Task]]] = None,\n) -&gt; None:\n\"\"\"\n    Initialize the benchmark.\n\n    Args:\n        languages: A list of languages to run the benchmark on. If None, all languages are used.\n        tasks: The tasks to run the benchmark on. If None, all tasks are used. Can either be specified as strings or as Task objects.\n    \"\"\"\n    self.languages = languages\n\n    self.tasks = self.get_tasks(tasks, languages)\n    self.task_names = [task.name for task in self.tasks]\n</code></pre>"},{"location":"api/#seb.benchmark.Benchmark.evaluate_model","title":"<code>evaluate_model(self, model, use_cache=True, run_model=True, raise_errors=True, cache_dir=None, verbose=True)</code>","text":"<p>Evaluate a model on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SebModel</code> <p>The model to evaluate.</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache.</p> <code>True</code> <code>run_model</code> <code>bool</code> <p>Whether to run the model if the cache is not present.</p> <code>True</code> <code>raise_errors</code> <code>bool</code> <p>Whether to raise errors.</p> <code>True</code> <code>cache_dir</code> <code>Optional[pathlib.Path]</code> <p>The cache directory to use. If None, the default cache directory is used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>The results of the benchmark.</p> Source code in <code>seb/benchmark.py</code> <pre><code>def evaluate_model(\n    self,\n    model: SebModel,\n    use_cache: bool = True,\n    run_model: bool = True,\n    raise_errors: bool = True,\n    cache_dir: Optional[Path] = None,\n    verbose: bool = True,\n) -&gt; BenchmarkResults:\n\"\"\"\n    Evaluate a model on the benchmark.\n\n    Args:\n        model: The model to evaluate.\n        use_cache: Whether to use the cache.\n        run_model: Whether to run the model if the cache is not present.\n        raise_errors: Whether to raise errors.\n        cache_dir: The cache directory to use. If None, the default cache directory is used.\n        verbose: Whether to show a progress bar.\n\n    Returns:\n        The results of the benchmark.\n    \"\"\"\n    task_results = []\n    pbar = tqdm(\n        self.tasks,\n        position=1,\n        desc=f\"Running {model.meta.name}\",\n        leave=False,\n        disable=not verbose,\n    )\n    for task in pbar:\n        pbar.set_description(f\"Running {model.meta.name} on {task.name}\")\n        task_result = run_task(\n            task,\n            model,\n            use_cache=use_cache,\n            run_model=run_model,\n            raise_errors=raise_errors,\n            cache_dir=cache_dir,\n        )\n        task_results.append(task_result)\n\n    return BenchmarkResults(meta=model.meta, task_results=task_results)\n</code></pre>"},{"location":"api/#seb.benchmark.Benchmark.evaluate_models","title":"<code>evaluate_models(self, models, use_cache=True, run_model=True, raise_errors=True, cache_dir=None, verbose=True)</code>","text":"<p>Evaluate a list of models on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[seb.interfaces.model.SebModel]</code> <p>The models to evaluate.</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use the cache.</p> <code>True</code> <code>run_model</code> <code>bool</code> <p>Whether to run the model if the cache is not present.</p> <code>True</code> <code>raise_errors</code> <code>bool</code> <p>Whether to raise errors.</p> <code>True</code> <code>cache_dir</code> <code>Optional[pathlib.Path]</code> <p>The cache directory to use. If None, the default cache directory is used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[seb.result_dataclasses.BenchmarkResults]</code> <p>The results of the benchmark, once for each model.</p> Source code in <code>seb/benchmark.py</code> <pre><code>def evaluate_models(\n    self,\n    models: list[SebModel],\n    use_cache: bool = True,\n    run_model: bool = True,\n    raise_errors: bool = True,\n    cache_dir: Optional[Path] = None,\n    verbose: bool = True,\n) -&gt; list[BenchmarkResults]:\n\"\"\"\n    Evaluate a list of models on the benchmark.\n\n    Args:\n        models: The models to evaluate.\n        use_cache: Whether to use the cache.\n        run_model: Whether to run the model if the cache is not present.\n        raise_errors: Whether to raise errors.\n        cache_dir: The cache directory to use. If None, the default cache directory is used.\n        verbose: Whether to show a progress bar.\n\n    Returns:\n        The results of the benchmark, once for each model.\n    \"\"\"\n    results = []\n    pbar = tqdm(\n        models,\n        position=0,\n        desc=\"Running Benchmark\",\n        leave=True,\n        disable=not verbose,\n    )\n\n    for model in pbar:\n        pbar.set_description(f\"Running {model.meta.name}\")\n        results.append(\n            self.evaluate_model(\n                model,\n                use_cache=use_cache,\n                run_model=run_model,\n                raise_errors=raise_errors,\n                cache_dir=cache_dir,\n                verbose=verbose,\n            ),\n        )\n    return results\n</code></pre>"},{"location":"api/#seb.benchmark.Benchmark.get_tasks","title":"<code>get_tasks(tasks, languages)</code>  <code>staticmethod</code>","text":"<p>Get the tasks for the benchmark.</p> <p>Returns:</p> Type Description <code>list[seb.interfaces.task.Task]</code> <p>A list of tasks.</p> Source code in <code>seb/benchmark.py</code> <pre><code>@staticmethod\ndef get_tasks(\n    tasks: Optional[Union[Iterable[str], Iterable[Task]]],\n    languages: Optional[list[str]],\n) -&gt; list[Task]:\n\"\"\"\n    Get the tasks for the benchmark.\n\n    Returns:\n        A list of tasks.\n    \"\"\"\n    _tasks = []\n\n    if tasks is None:\n        _tasks = get_all_tasks()\n    else:\n        for task in tasks:\n            if isinstance(task, str):\n                _tasks.append(get_task(task))\n            elif isinstance(task, Task):\n                _tasks.append(task)\n            else:\n                raise ValueError(f\"Invalid task type: {type(task)}\")\n\n    if languages is not None:\n        langs = set(languages)\n        _tasks = [task for task in _tasks if set(task.languages) &amp; langs]\n\n    return _tasks\n</code></pre>"},{"location":"api/#interfaces","title":"Interfaces","text":"<p>SEB implements to main interfaces. A task interface which is a tasks within the Benchmark and a model interface which is a model applied to the tasks.</p>"},{"location":"api/#model-interface","title":"Model Interface","text":""},{"location":"api/#seb.interfaces.model.Encoder","title":"<code> seb.interfaces.model.Encoder            (Protocol)         </code>","text":"<p>Interface which all models must implement.</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>@runtime_checkable\nclass Encoder(Protocol):\n\"\"\"\n    Interface which all models must implement.\n    \"\"\"\n\n    def encode(\n        self,\n        sentences: list[str],\n        *,\n        task: Optional[\"Task\"] = None,\n        batch_size: int = 32,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n\"\"\"Returns a list of embeddings for the given sentences.\n\n        Args:\n            sentences: List of sentences to encode\n            task: The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need\n                to be used.\n            batch_size: Batch size for the encoding\n            kwargs: arguments to pass to the models encode method\n\n        Returns:\n            Embeddings for the given documents\n        \"\"\"\n        ...\n\n    # The following methods are optional and can be implemented if the model supports them.\n    # def to(self, device: torch.device):\n    #     ...\n\n    # def encode_queries(self, queries: list[str], **kwargs: Any) -&gt; np.ndarray:\n    #     ...\n\n    # def encode_corpus(self, corpus: list[dict[str, str]], **kwargs: Any) -&gt; np.ndarray:\n    #     ...\n</code></pre>"},{"location":"api/#seb.interfaces.model.Encoder.encode","title":"<code>encode(self, sentences, *, task=None, batch_size=32, **kwargs)</code>","text":"<p>Returns a list of embeddings for the given sentences.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>list[str]</code> <p>List of sentences to encode</p> required <code>task</code> <code>Optional[Task]</code> <p>The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need to be used.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for the encoding</p> <code>32</code> <code>kwargs</code> <code>Any</code> <p>arguments to pass to the models encode method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Embeddings for the given documents</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>def encode(\n    self,\n    sentences: list[str],\n    *,\n    task: Optional[\"Task\"] = None,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n\"\"\"Returns a list of embeddings for the given sentences.\n\n    Args:\n        sentences: List of sentences to encode\n        task: The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need\n            to be used.\n        batch_size: Batch size for the encoding\n        kwargs: arguments to pass to the models encode method\n\n    Returns:\n        Embeddings for the given documents\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#seb.interfaces.model.LazyLoadEncoder","title":"<code> seb.interfaces.model.LazyLoadEncoder            (Encoder)         </code>  <code>dataclass</code>","text":"<p>Encoder object, which lazy loads the model on the first call to encode()</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>@dataclass\nclass LazyLoadEncoder(Encoder):\n\"\"\"Encoder object, which lazy loads the model on the first call to encode()\"\"\"\n\n    loader: Callable[[], Encoder]\n    _model: Optional[Encoder] = None\n\n    def load_model(self):\n\"\"\"\n        Load the model.\n        \"\"\"\n        if self._model is None:\n            self._model = self.loader()\n\n    def to(self, device: torch.device):\n        self.load_model()\n        try:\n            self._model = self._model.to(device)  # type: ignore\n        except AttributeError:\n            logging.debug(f\"Model {self._model} does not have a to method\")\n\n    @property\n    def model(self) -&gt; Encoder:\n\"\"\"\n        Dynimically load the model.\n        \"\"\"\n        self.load_model()\n        return self._model  # type: ignore\n\n    def encode(\n        self,\n        sentences: list[str],\n        *,\n        task: Optional[\"Task\"] = None,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n\"\"\"\n        Returns a list of embeddings for the given sentences.\n        Args:\n            sentences: List of sentences to encode\n            task: The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need\n                to be used.\n            batch_size: Batch size for the encoding\n            kwargs: arguments to pass to the models encode method\n\n        Returns:\n            Embeddings for the given documents\n        \"\"\"\n        return self.model.encode(sentences, task=task, **kwargs)\n\n    def encode_queries(self, queries: list[str], **kwargs: Any) -&gt; np.ndarray:\n        try:\n            return self.model.encode_queries(queries, **kwargs)  # type: ignore\n        except AttributeError:\n            return self.encode(queries, **kwargs)\n\n    def encode_corpus(self, corpus: list[dict[str, str]], **kwargs: Any) -&gt; np.ndarray:\n        try:\n            return self.model.encode_corpus(corpus, **kwargs)  # type: ignore\n        except AttributeError:\n            sep = \" \"\n            if isinstance(corpus, dict):\n                sentences = [\n                    (corpus[\"title\"][i] + sep + corpus[\"text\"][i]).strip() if \"title\" in corpus else corpus[\"text\"][i].strip()  # type: ignore\n                    for i in range(len(corpus[\"text\"]))  # type: ignore\n                ]\n            else:\n                sentences = [(doc[\"title\"] + sep + doc[\"text\"]).strip() if \"title\" in doc else doc[\"text\"].strip() for doc in corpus]\n            return self.encode(sentences, **kwargs)\n</code></pre>"},{"location":"api/#seb.interfaces.model.LazyLoadEncoder.model","title":"<code>model: Encoder</code>  <code>property</code> <code>readonly</code>","text":"<p>Dynimically load the model.</p>"},{"location":"api/#seb.interfaces.model.LazyLoadEncoder.__init__","title":"<code>__init__(self, loader, _model=None)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/#seb.interfaces.model.LazyLoadEncoder.encode","title":"<code>encode(self, sentences, *, task=None, **kwargs)</code>","text":"<p>Returns a list of embeddings for the given sentences.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>list[str]</code> <p>List of sentences to encode</p> required <code>task</code> <code>Optional[Task]</code> <p>The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need to be used.</p> <code>None</code> <code>batch_size</code> <p>Batch size for the encoding</p> required <code>kwargs</code> <code>Any</code> <p>arguments to pass to the models encode method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Embeddings for the given documents</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>def encode(\n    self,\n    sentences: list[str],\n    *,\n    task: Optional[\"Task\"] = None,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n\"\"\"\n    Returns a list of embeddings for the given sentences.\n    Args:\n        sentences: List of sentences to encode\n        task: The task to encode for. This allows the model to encode differently for different tasks. Will always be given but does not need\n            to be used.\n        batch_size: Batch size for the encoding\n        kwargs: arguments to pass to the models encode method\n\n    Returns:\n        Embeddings for the given documents\n    \"\"\"\n    return self.model.encode(sentences, task=task, **kwargs)\n</code></pre>"},{"location":"api/#seb.interfaces.model.LazyLoadEncoder.load_model","title":"<code>load_model(self)</code>","text":"<p>Load the model.</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>def load_model(self):\n\"\"\"\n    Load the model.\n    \"\"\"\n    if self._model is None:\n        self._model = self.loader()\n</code></pre>"},{"location":"api/#seb.interfaces.model.SebModel","title":"<code> seb.interfaces.model.SebModel        </code>  <code>dataclass</code>","text":"<p>An embedding model as implemented in SEB. It notably dynamically loads models (such that models are not loaded when a cache is hit) and includes metadata pertaining to the specific model.</p> Source code in <code>seb/interfaces/model.py</code> <pre><code>@dataclass\nclass SebModel:\n\"\"\"\n    An embedding model as implemented in SEB. It notably dynamically loads models (such that models are not loaded when a cache is hit)\n    and includes metadata pertaining to the specific model.\n    \"\"\"\n\n    meta: ModelMeta\n    encoder: Encoder\n\n    @property\n    def number_of_parameters(self) -&gt; Optional[int]:\n\"\"\"\n        Returns the number of parameters in the model.\n        \"\"\"\n        if hasattr(self.encoder, \"num_parameters\"):\n            return sum(p.numel() for p in self.model.parameters() if p.requires_grad)  # type: ignore\n        return None\n</code></pre>"},{"location":"api/#seb.interfaces.model.SebModel.number_of_parameters","title":"<code>number_of_parameters: Optional[int]</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the number of parameters in the model.</p>"},{"location":"api/#task-interface","title":"Task Interface","text":""},{"location":"api/#seb.interfaces.task.Task","title":"<code> seb.interfaces.task.Task            (Protocol)         </code>","text":"<p>A task is a specific evaluation task for a sentence embedding model.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the task.</p> <code>main_score</code> <code>str</code> <p>The main score of the task.</p> <code>reference</code> <code>str</code> <p>A reference to the task.</p> <code>version</code> <code>str</code> <p>The version of the task.</p> <code>languages</code> <code>list[Literal['da', 'nb', 'nn', 'sv', 'da-bornholm', 'is', 'fo', 'en']]</code> <p>The languages of the task.</p> <code>domain</code> <code>list[Literal['social', 'poetry', 'wiki', 'fiction', 'non-fiction', 'web', 'legal', 'news', 'academic', 'spoken', 'reviews', 'blog', 'medical', 'government', 'bible']]</code> <p>The domains of the task. Should be one of the categories listed on https://universaldependencies.org</p> <code>task_type</code> <code>Literal['Classification', 'Retrieval', 'STS', 'BitextMining', 'Clustering', 'Speed']</code> <p>A list of task types, determines how the task is being evaluated. E.g. Classification.</p> <code>task_subtypes</code> <code>list[str]</code> <p>a list of subtypes e.g. Sentiment Classification.</p> <code>description</code> <code>str</code> <p>A description of the task.</p> Source code in <code>seb/interfaces/task.py</code> <pre><code>@runtime_checkable\nclass Task(Protocol):\n\"\"\"A task is a specific evaluation task for a sentence embedding model.\n\n    Attributes:\n        name: The name of the task.\n        main_score: The main score of the task.\n        reference: A reference to the task.\n        version: The version of the task.\n        languages: The languages of the task.\n        domain: The domains of the task. Should be one of the categories listed on https://universaldependencies.org\n        task_type: A list of task types, determines how the task is being evaluated. E.g. Classification.\n        task_subtypes: a list of subtypes e.g. Sentiment Classification.\n        description: A description of the task.\n    \"\"\"\n\n    name: str\n    main_score: str\n    reference: str\n    version: str\n    languages: list[Language]\n    domain: list[Domain]\n    task_type: TaskType\n    task_subtypes: list[str]\n    description: str\n\n    def evaluate(self, model: Encoder) -&gt; TaskResult:\n\"\"\"Evaluates a Sentence Embedding Model on the task.\n\n        Args:\n            model: A model with the encode method implemented.\n\n        Returns:\n            A TaskResult object.\n        \"\"\"\n        ...\n\n    def get_documents(self) -&gt; list[str]:\n\"\"\"Get the documents for the task.\n\n        Returns:\n            A list of strings.\n        \"\"\"\n        ...\n\n    def get_descriptive_stats(self) -&gt; DescriptiveDatasetStats:\n        texts = self.get_documents()\n        document_lengths = np.array([len(text) for text in texts])\n\n        mean = float(np.mean(document_lengths))\n        std = float(np.std(document_lengths))\n        return DescriptiveDatasetStats(\n            mean_document_length=mean,\n            std_document_length=std,\n            num_documents=len(document_lengths),\n        )\n\n    def name_to_path(self) -&gt; str:\n\"\"\"Convert a name to a path.\"\"\"\n        name = self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n        return name\n</code></pre>"},{"location":"api/#seb.interfaces.task.Task.evaluate","title":"<code>evaluate(self, model)</code>","text":"<p>Evaluates a Sentence Embedding Model on the task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Encoder</code> <p>A model with the encode method implemented.</p> required <p>Returns:</p> Type Description <code>TaskResult</code> <p>A TaskResult object.</p> Source code in <code>seb/interfaces/task.py</code> <pre><code>def evaluate(self, model: Encoder) -&gt; TaskResult:\n\"\"\"Evaluates a Sentence Embedding Model on the task.\n\n    Args:\n        model: A model with the encode method implemented.\n\n    Returns:\n        A TaskResult object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#seb.interfaces.task.Task.get_documents","title":"<code>get_documents(self)</code>","text":"<p>Get the documents for the task.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of strings.</p> Source code in <code>seb/interfaces/task.py</code> <pre><code>def get_documents(self) -&gt; list[str]:\n\"\"\"Get the documents for the task.\n\n    Returns:\n        A list of strings.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#seb.interfaces.task.Task.name_to_path","title":"<code>name_to_path(self)</code>","text":"<p>Convert a name to a path.</p> Source code in <code>seb/interfaces/task.py</code> <pre><code>def name_to_path(self) -&gt; str:\n\"\"\"Convert a name to a path.\"\"\"\n    name = self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n    return name\n</code></pre>"},{"location":"api/#data-classes","title":"Data Classes","text":"<p>SEB uses data classes to store the results of a benchmark. The following classes are available:</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults","title":"<code> seb.result_dataclasses.BenchmarkResults            (BaseModel)         </code>","text":"<p>Dataclass for storing benchmark results.</p> <p>Attributes:</p> Name Type Description <code>meta</code> <code>ModelMeta</code> <p>ModelMeta object.</p> <code>task_results</code> <code>list[Union[seb.result_dataclasses.TaskResult, seb.result_dataclasses.TaskError]]</code> <p>List of TaskResult objects.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>class BenchmarkResults(BaseModel):\n\"\"\"\n    Dataclass for storing benchmark results.\n\n    Attributes:\n        meta: ModelMeta object.\n        task_results: List of TaskResult objects.\n    \"\"\"\n\n    meta: ModelMeta\n    task_results: list[Union[TaskResult, TaskError]]\n\n    def get_main_score(self, lang: Optional[Iterable[Language]] = None) -&gt; float:\n        scores = [t.get_main_score(lang) for t in self.task_results]\n        if scores:\n            return sum(scores) / len(scores)\n        return np.nan\n\n    def __iter__(self) -&gt; Iterator[Union[TaskResult, TaskError]]:  # type: ignore\n        return iter(self.task_results)\n\n    def __getitem__(self, index: int) -&gt; Union[TaskResult, TaskError]:\n        return self.task_results[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.task_results)\n\n    def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n        Write task results to a path.\n        \"\"\"\n        if path.is_file():\n            raise ValueError(\"Can't save BenchmarkResults to a file. Path must be a directory.\")\n        path.mkdir(parents=True, exist_ok=True)\n        for task_result in self.task_results:\n            if isinstance(task_result, TaskResult):\n                task_result.to_disk(path / f\"{task_result.task_name}.json\")\n            else:\n                task_result.to_disk(path / f\"{task_result.task_name}.error.json\")\n\n        meta_path = path / \"meta.json\"\n        self.meta.to_disk(meta_path)\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; \"BenchmarkResults\":\n\"\"\"\n        Load task results from a path.\n        \"\"\"\n        if not path.is_dir():\n            raise ValueError(\"Can't load BenchmarkResults from path: {path}. Path must be a directory.\")\n        task_results = []\n        for file in path.glob(\"*.json\"):\n            if file.stem == \"meta\":\n                continue\n            if file.stem.endswith(\".error\"):\n                task_results.append(TaskError.from_disk(file))\n            else:\n                task_results.append(TaskResult.from_disk(file))\n\n        meta_path = path / \"meta.json\"\n        meta = ModelMeta.from_disk(meta_path)\n        return cls(meta=meta, task_results=task_results)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__class_vars__","title":"<code>__class_vars__</code>  <code>special</code>","text":"<p>The names of the class variables defined on the model.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__private_attributes__","title":"<code>__private_attributes__</code>  <code>special</code>","text":"<p>Metadata about the private attributes of the model.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_complete__","title":"<code>__pydantic_complete__</code>  <code>special</code>","text":"<p>Whether model building is completed, or if there are still undefined fields.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_computed_fields__","title":"<code>__pydantic_computed_fields__</code>  <code>special</code>","text":"<p>A dictionary of computed field names and their corresponding [<code>ComputedFieldInfo</code>][pydantic.fields.ComputedFieldInfo] objects.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_custom_init__","title":"<code>__pydantic_custom_init__</code>  <code>special</code>","text":"<p>Whether the model has a custom <code>__init__</code> method.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_decorators__","title":"<code>__pydantic_decorators__</code>  <code>special</code>","text":"<p>Metadata containing the decorators defined on the model. This replaces <code>Model.__validators__</code> and <code>Model.__root_validators__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_fields__","title":"<code>__pydantic_fields__</code>  <code>special</code>","text":"<p>A dictionary of field names and their corresponding [<code>FieldInfo</code>][pydantic.fields.FieldInfo] objects. This replaces <code>Model.__fields__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_generic_metadata__","title":"<code>__pydantic_generic_metadata__</code>  <code>special</code>","text":"<p>Metadata for generic models; contains data used for a similar purpose to args, origin, parameters in typing-module generics. May eventually be replaced by these.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_parent_namespace__","title":"<code>__pydantic_parent_namespace__</code>  <code>special</code>","text":"<p>Parent namespace of the model, used for automatic rebuilding of models.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_post_init__","title":"<code>__pydantic_post_init__</code>  <code>special</code>","text":"<p>The name of the post-init method for the model, if defined.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__pydantic_setattr_handlers__","title":"<code>__pydantic_setattr_handlers__</code>  <code>special</code>","text":"<p><code>__setattr__</code> handlers. Memoizing the handlers leads to a dramatic performance improvement in <code>__setattr__</code></p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.__signature__","title":"<code>__signature__</code>  <code>special</code>","text":"<p>The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.model_config","title":"<code>model_config</code>","text":"<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load task results from a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; \"BenchmarkResults\":\n\"\"\"\n    Load task results from a path.\n    \"\"\"\n    if not path.is_dir():\n        raise ValueError(\"Can't load BenchmarkResults from path: {path}. Path must be a directory.\")\n    task_results = []\n    for file in path.glob(\"*.json\"):\n        if file.stem == \"meta\":\n            continue\n        if file.stem.endswith(\".error\"):\n            task_results.append(TaskError.from_disk(file))\n        else:\n            task_results.append(TaskResult.from_disk(file))\n\n    meta_path = path / \"meta.json\"\n    meta = ModelMeta.from_disk(meta_path)\n    return cls(meta=meta, task_results=task_results)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.BenchmarkResults.to_disk","title":"<code>to_disk(self, path)</code>","text":"<p>Write task results to a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n    Write task results to a path.\n    \"\"\"\n    if path.is_file():\n        raise ValueError(\"Can't save BenchmarkResults to a file. Path must be a directory.\")\n    path.mkdir(parents=True, exist_ok=True)\n    for task_result in self.task_results:\n        if isinstance(task_result, TaskResult):\n            task_result.to_disk(path / f\"{task_result.task_name}.json\")\n        else:\n            task_result.to_disk(path / f\"{task_result.task_name}.error.json\")\n\n    meta_path = path / \"meta.json\"\n    self.meta.to_disk(meta_path)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskResult","title":"<code> seb.result_dataclasses.TaskResult            (BaseModel)         </code>","text":"<p>Dataclass for storing task results.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>Name of the task.</p> <code>task_description</code> <code>str</code> <p>Description of the task.</p> <code>task_version</code> <code>str</code> <p>Version of the task.</p> <code>time_of_run</code> <code>datetime</code> <p>Time of the run.</p> <code>scores</code> <code>dict[Literal['da', 'nb', 'nn', 'sv', 'da-bornholm', 'is', 'fo', 'en'], dict[str, Union[float, str]]]</code> <p>Dictionary of scores on the form {language: {\"metric\": value}}.</p> <code>main_score</code> <code>str</code> <p>Name of the main score.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>class TaskResult(BaseModel):\n\"\"\"\n    Dataclass for storing task results.\n\n    Attributes:\n        task_name: Name of the task.\n        task_description: Description of the task.\n        task_version: Version of the task.\n        time_of_run: Time of the run.\n        scores: Dictionary of scores on the form {language: {\"metric\": value}}.\n        main_score: Name of the main score.\n    \"\"\"\n\n    task_name: str\n    task_description: str\n    task_version: str\n    time_of_run: datetime\n    scores: dict[Language, dict[str, Union[float, str]]]  # {language: {\"metric\": value}}.\n    main_score: str\n\n    def get_main_score(self, lang: Optional[Iterable[str]] = None) -&gt; float:\n\"\"\"\n        Returns the main score for a given set of languages.\n\n        Args:\n            lang: List of languages to get the main score for.\n\n        Returns:\n            The main score.\n        \"\"\"\n        main_scores = []\n        if lang is None:\n            lang = self.scores.keys()\n\n        for l in lang:\n            main_scores.append(self.scores[l][self.main_score])  # type: ignore\n\n        return sum(main_scores) / len(main_scores)\n\n    @property\n    def languages(self) -&gt; list[Language]:\n\"\"\"\n        Returns the languages of the task.\n        \"\"\"\n        return list(self.scores.keys())\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; \"TaskResult\":\n\"\"\"\n        Load task results from a path.\n        \"\"\"\n        with path.open(\"r\") as f:\n            task_results = json.load(f)\n        return cls(**task_results)\n\n    def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n        Write task results to a path.\n        \"\"\"\n        path.parent.mkdir(parents=True, exist_ok=True)\n        json_str: str = self.model_dump_json()  # type: ignore\n\n        with path.open(\"w\") as f:\n            f.write(json_str)\n\n    def name_to_path(self) -&gt; str:\n\"\"\"\n        Convert a name to a path.\n        \"\"\"\n        name = self.task_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n        return name\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskResult.__class_vars__","title":"<code>__class_vars__</code>  <code>special</code>","text":"<p>The names of the class variables defined on the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__private_attributes__","title":"<code>__private_attributes__</code>  <code>special</code>","text":"<p>Metadata about the private attributes of the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_complete__","title":"<code>__pydantic_complete__</code>  <code>special</code>","text":"<p>Whether model building is completed, or if there are still undefined fields.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_computed_fields__","title":"<code>__pydantic_computed_fields__</code>  <code>special</code>","text":"<p>A dictionary of computed field names and their corresponding [<code>ComputedFieldInfo</code>][pydantic.fields.ComputedFieldInfo] objects.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_custom_init__","title":"<code>__pydantic_custom_init__</code>  <code>special</code>","text":"<p>Whether the model has a custom <code>__init__</code> method.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_decorators__","title":"<code>__pydantic_decorators__</code>  <code>special</code>","text":"<p>Metadata containing the decorators defined on the model. This replaces <code>Model.__validators__</code> and <code>Model.__root_validators__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_fields__","title":"<code>__pydantic_fields__</code>  <code>special</code>","text":"<p>A dictionary of field names and their corresponding [<code>FieldInfo</code>][pydantic.fields.FieldInfo] objects. This replaces <code>Model.__fields__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_generic_metadata__","title":"<code>__pydantic_generic_metadata__</code>  <code>special</code>","text":"<p>Metadata for generic models; contains data used for a similar purpose to args, origin, parameters in typing-module generics. May eventually be replaced by these.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_parent_namespace__","title":"<code>__pydantic_parent_namespace__</code>  <code>special</code>","text":"<p>Parent namespace of the model, used for automatic rebuilding of models.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_post_init__","title":"<code>__pydantic_post_init__</code>  <code>special</code>","text":"<p>The name of the post-init method for the model, if defined.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__pydantic_setattr_handlers__","title":"<code>__pydantic_setattr_handlers__</code>  <code>special</code>","text":"<p><code>__setattr__</code> handlers. Memoizing the handlers leads to a dramatic performance improvement in <code>__setattr__</code></p>"},{"location":"api/#seb.result_dataclasses.TaskResult.__signature__","title":"<code>__signature__</code>  <code>special</code>","text":"<p>The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.languages","title":"<code>languages: list[Literal['da', 'nb', 'nn', 'sv', 'da-bornholm', 'is', 'fo', 'en']]</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the languages of the task.</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.model_config","title":"<code>model_config</code>","text":"<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>"},{"location":"api/#seb.result_dataclasses.TaskResult.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load task results from a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; \"TaskResult\":\n\"\"\"\n    Load task results from a path.\n    \"\"\"\n    with path.open(\"r\") as f:\n        task_results = json.load(f)\n    return cls(**task_results)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskResult.get_main_score","title":"<code>get_main_score(self, lang=None)</code>","text":"<p>Returns the main score for a given set of languages.</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>Optional[collections.abc.Iterable[str]]</code> <p>List of languages to get the main score for.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The main score.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def get_main_score(self, lang: Optional[Iterable[str]] = None) -&gt; float:\n\"\"\"\n    Returns the main score for a given set of languages.\n\n    Args:\n        lang: List of languages to get the main score for.\n\n    Returns:\n        The main score.\n    \"\"\"\n    main_scores = []\n    if lang is None:\n        lang = self.scores.keys()\n\n    for l in lang:\n        main_scores.append(self.scores[l][self.main_score])  # type: ignore\n\n    return sum(main_scores) / len(main_scores)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskResult.name_to_path","title":"<code>name_to_path(self)</code>","text":"<p>Convert a name to a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def name_to_path(self) -&gt; str:\n\"\"\"\n    Convert a name to a path.\n    \"\"\"\n    name = self.task_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n    return name\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskResult.to_disk","title":"<code>to_disk(self, path)</code>","text":"<p>Write task results to a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n    Write task results to a path.\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    json_str: str = self.model_dump_json()  # type: ignore\n\n    with path.open(\"w\") as f:\n        f.write(json_str)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskError","title":"<code> seb.result_dataclasses.TaskError            (BaseModel)         </code>","text":"Source code in <code>seb/result_dataclasses.py</code> <pre><code>class TaskError(BaseModel):\n    task_name: str\n    error: str\n    time_of_run: datetime\n    languages: list[str] = []\n\n    def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n        Write task results to a path.\n        \"\"\"\n        path.parent.mkdir(parents=True, exist_ok=True)\n        json_str: str = self.model_dump_json()  # type: ignore\n\n        with path.open(\"w\") as f:\n            f.write(json_str)\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; \"TaskError\":\n\"\"\"\n        Load task results from a path.\n        \"\"\"\n        with path.open() as f:\n            task_results = json.load(f)\n        return cls(**task_results)\n\n    @staticmethod\n    def get_main_score(lang: Optional[Iterable[str]] = None) -&gt; float:  # noqa: ARG004\n        return np.nan\n\n    def name_to_path(self) -&gt; str:\n\"\"\"\n        Convert a name to a path.\n        \"\"\"\n        name = self.task_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n        return name\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskError.__class_vars__","title":"<code>__class_vars__</code>  <code>special</code>","text":"<p>The names of the class variables defined on the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__private_attributes__","title":"<code>__private_attributes__</code>  <code>special</code>","text":"<p>Metadata about the private attributes of the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_complete__","title":"<code>__pydantic_complete__</code>  <code>special</code>","text":"<p>Whether model building is completed, or if there are still undefined fields.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_computed_fields__","title":"<code>__pydantic_computed_fields__</code>  <code>special</code>","text":"<p>A dictionary of computed field names and their corresponding [<code>ComputedFieldInfo</code>][pydantic.fields.ComputedFieldInfo] objects.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_custom_init__","title":"<code>__pydantic_custom_init__</code>  <code>special</code>","text":"<p>Whether the model has a custom <code>__init__</code> method.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_decorators__","title":"<code>__pydantic_decorators__</code>  <code>special</code>","text":"<p>Metadata containing the decorators defined on the model. This replaces <code>Model.__validators__</code> and <code>Model.__root_validators__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_fields__","title":"<code>__pydantic_fields__</code>  <code>special</code>","text":"<p>A dictionary of field names and their corresponding [<code>FieldInfo</code>][pydantic.fields.FieldInfo] objects. This replaces <code>Model.__fields__</code> from Pydantic V1.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_generic_metadata__","title":"<code>__pydantic_generic_metadata__</code>  <code>special</code>","text":"<p>Metadata for generic models; contains data used for a similar purpose to args, origin, parameters in typing-module generics. May eventually be replaced by these.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_parent_namespace__","title":"<code>__pydantic_parent_namespace__</code>  <code>special</code>","text":"<p>Parent namespace of the model, used for automatic rebuilding of models.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_post_init__","title":"<code>__pydantic_post_init__</code>  <code>special</code>","text":"<p>The name of the post-init method for the model, if defined.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.__pydantic_setattr_handlers__","title":"<code>__pydantic_setattr_handlers__</code>  <code>special</code>","text":"<p><code>__setattr__</code> handlers. Memoizing the handlers leads to a dramatic performance improvement in <code>__setattr__</code></p>"},{"location":"api/#seb.result_dataclasses.TaskError.__signature__","title":"<code>__signature__</code>  <code>special</code>","text":"<p>The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</p>"},{"location":"api/#seb.result_dataclasses.TaskError.model_config","title":"<code>model_config</code>","text":"<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>"},{"location":"api/#seb.result_dataclasses.TaskError.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load task results from a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; \"TaskError\":\n\"\"\"\n    Load task results from a path.\n    \"\"\"\n    with path.open() as f:\n        task_results = json.load(f)\n    return cls(**task_results)\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskError.name_to_path","title":"<code>name_to_path(self)</code>","text":"<p>Convert a name to a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def name_to_path(self) -&gt; str:\n\"\"\"\n    Convert a name to a path.\n    \"\"\"\n    name = self.task_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n    return name\n</code></pre>"},{"location":"api/#seb.result_dataclasses.TaskError.to_disk","title":"<code>to_disk(self, path)</code>","text":"<p>Write task results to a path.</p> Source code in <code>seb/result_dataclasses.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n\"\"\"\n    Write task results to a path.\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    json_str: str = self.model_dump_json()  # type: ignore\n\n    with path.open(\"w\") as f:\n        f.write(json_str)\n</code></pre>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#command-line-interface","title":"Command Line Interface","text":"<p>Documentation for the command line interface of SEB.</p>"},{"location":"cli/#cli","title":"CLI","text":""},{"location":"cli/#run","title":"<code>run</code>","text":"<p>Runs the Benchmark either on specified models or on all registered models.  Can save the benchmark's results, but also displays them in a table similar to the official website. </p> <p>Examples: To run all models on all languages and tasks: </p> <p><code>{bash} seb run</code> </p> <p>To run a model on all languages and tasks: </p> <pre><code>seb run -m sentence-transformers/all-MiniLM-L6-v2\n</code></pre> <p>To run multiple models: To run a model on all languages and tasks: </p> <pre><code>seb run -m sentence-transformers/all-MiniLM-L6-v2,sentence-transformers/all-mpnet-base-v2\n</code></pre> <p>if you only want to limit it to a subset of languages or tasks you can use the <code>--languages</code> and <code>--tasks</code> flags.</p> <pre><code># Running a model on a subset of languages \nseb run sentence-transformers/all-MiniLM-L6-v2 -o results/ -l nb,nn \n# Running a model on a subset of tasks \nseb run sentence-transformers/all-MiniLM-L6-v2 -o results/ -t DKHate,ScaLA\n</code></pre> Argument Type Description Default <code>--models</code>, <code>-m</code> <code>Optional[list[str], NoneType]</code> Model names or paths. If a model is not registrered in SEB it will be loaded using SentenceTransformers. If none are specified the whole benchmark is run. <code>None</code> <code>--output-path</code>, <code>-o</code> <code>Path</code> Directory to save all results to. <code>None</code> <code>--languages</code>, <code>-l</code> <code>Optional[list[str], NoneType]</code> What languages subsection to run the benchmark on. If left blank it will run it on all languages. <code>None</code> <code>--tasks</code>, <code>-t</code> <code>Optional[list[str], NoneType]</code> What tasks should model be run on. Default to all tasks within the specified languages. <code>None</code> <code>--ignore-cache</code> <code>bool</code> Ignores caches models. Note that SEB ships with an existing cache. You can set the cache_dir using the environmental variable SEB_CACHE_DIR <code>False</code> <code>--ignore-errors</code> <code>bool</code> Should errors be ignored when running a model on a benchmark task. <code>False</code> <code>--code</code>, <code>-c</code> <code>Path</code> Code to run before executing benchmark. Useful for adding custom model to registries. <code>None</code> <code>--logging-level</code> <code>str</code> Logging level for the benchmark. <code>'INFO'</code>"},{"location":"create_cli_docs/","title":"Create cli docs","text":"In\u00a0[\u00a0]: Copied! <pre># This just created a rough draft of the CLI documentation. It is not\n# intended to be used for anything other than a starting point.\n# at least we would need this issue fixed first:\n# https://github.com/explosion/radicli/issues/30\nfrom __future__ import annotations\n</pre> # This just created a rough draft of the CLI documentation. It is not # intended to be used for anything other than a starting point. # at least we would need this issue fixed first: # https://github.com/explosion/radicli/issues/30 from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>from seb.cli import cli\n</pre> from seb.cli import cli In\u00a0[\u00a0]: Copied! <pre>title = \"Command Line Interface\"\ndescription = \"Documentation for the command line interface of SEB.\"\n</pre> title = \"Command Line Interface\" description = \"Documentation for the command line interface of SEB.\" In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    with Path(\"docs/cli.md\").open(\"w\", encoding=\"utf8\") as f:\n        f.write(cli.document(title=title, description=description))\n</pre> if __name__ == \"__main__\":     with Path(\"docs/cli.md\").open(\"w\", encoding=\"utf8\") as f:         f.write(cli.document(title=title, description=description))"},{"location":"create_desc_stats/","title":"Create desc stats","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom tqdm import tqdm\n</pre> import pandas as pd from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>from seb import registries\n</pre> from seb import registries In\u00a0[\u00a0]: Copied! <pre>def insert_table(file: Path, table: str) -&gt; None:\n    # Read the original Markdown file\n    with file.open(\"r\") as f:\n        content = f.read()\n\n    # Split the content at the start and end placeholders\n    start_section, end_section = (\n        content.split(\"&lt;!--START_TABLE--&gt;\")[0],\n        content.split(\"&lt;!--END_TABLE--&gt;\")[1],\n    )\n\n    # Construct the updated content\n    updated_content = start_section + \"&lt;!--START_TABLE--&gt;\\n\" + table + \"\\n&lt;!--END_TABLE--&gt;\" + end_section\n\n    # Write the updated content back to the file\n    with file.open(\"w\") as f:\n        f.write(updated_content)\n</pre> def insert_table(file: Path, table: str) -&gt; None:     # Read the original Markdown file     with file.open(\"r\") as f:         content = f.read()      # Split the content at the start and end placeholders     start_section, end_section = (         content.split(\"\")[0],         content.split(\"\")[1],     )      # Construct the updated content     updated_content = start_section + \"\\n\" + table + \"\\n\" + end_section      # Write the updated content back to the file     with file.open(\"w\") as f:         f.write(updated_content) In\u00a0[\u00a0]: Copied! <pre>def create_table() -&gt; pd.DataFrame:\n    # Initialize an empty DataFrame\n    columns = [\n        \"Dataset\",\n        \"Description\",\n        \"Main Score\",\n        \"Languages\",\n        \"Type\",\n        \"Domains\",\n        \"Number of Documents\",\n        \"Mean Length of Documents (characters)\",\n    ]\n    data = []\n\n    tasks = registries.tasks.get_all().items()\n    tasks = sorted(tasks, key=lambda x: x[0])\n\n    pbar = tqdm(tasks, desc=\"Creating Table\")\n    for name, getter in pbar:\n        pbar.set_postfix_str(name)\n        task = getter()\n        stats = task.get_descriptive_stats()\n\n        # Create a row for each task\n        row = [\n            f\"[{task.name}]({task.reference})\",  # Dataset with hyperlink\n            task.description,  # Description\n            task.main_score.capitalize(),  # Main Score\n            \", \".join(task.languages),  # Languages\n            task.task_type,  # Type\n            \", \".join(task.domain),  # Domains\n            stats[\"num_documents\"],  # Number of Documents\n            f'{stats[\"mean_document_length\"]:.2f} (std: {stats[\"std_document_length\"]:.2f})',  # Mean Length of Documents\n        ]\n        data.append(row)\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data, columns=columns)\n\n    return df\n</pre> def create_table() -&gt; pd.DataFrame:     # Initialize an empty DataFrame     columns = [         \"Dataset\",         \"Description\",         \"Main Score\",         \"Languages\",         \"Type\",         \"Domains\",         \"Number of Documents\",         \"Mean Length of Documents (characters)\",     ]     data = []      tasks = registries.tasks.get_all().items()     tasks = sorted(tasks, key=lambda x: x[0])      pbar = tqdm(tasks, desc=\"Creating Table\")     for name, getter in pbar:         pbar.set_postfix_str(name)         task = getter()         stats = task.get_descriptive_stats()          # Create a row for each task         row = [             f\"[{task.name}]({task.reference})\",  # Dataset with hyperlink             task.description,  # Description             task.main_score.capitalize(),  # Main Score             \", \".join(task.languages),  # Languages             task.task_type,  # Type             \", \".join(task.domain),  # Domains             stats[\"num_documents\"],  # Number of Documents             f'{stats[\"mean_document_length\"]:.2f} (std: {stats[\"std_document_length\"]:.2f})',  # Mean Length of Documents         ]         data.append(row)      # Convert the data to a DataFrame     df = pd.DataFrame(data, columns=columns)      return df In\u00a0[\u00a0]: Copied! <pre>def main():\n    path = Path(__file__).parent / \"datasets.md\"\n\n    df = create_table()\n    insert_table(path, df.to_markdown(index=False))\n</pre> def main():     path = Path(__file__).parent / \"datasets.md\"      df = create_table()     insert_table(path, df.to_markdown(index=False)) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#descriptions","title":"Descriptions","text":"<p>The following tables contains description of all the dataset in the benchmark along with with their main score, what type of task it as, what languages it covers and some statistics for each dataset. The domains follows the categories used in the Universal Dependencies project.</p> Dataset Description Main Score Languages Type Domains Number of Documents Mean Length of Documents (characters) Angry Tweets A sentiment dataset with 3 classes (positiv, negativ, neutral) for Danish tweets Accuracy da Classification social 1047 156.15 (std: 82.02) Bornholm Parallel Danish Bornholmsk Parallel Corpus. Bornholmsk is a Danish dialect spoken on the island of Bornholm, Denmark. Historically it is a part of east Danish which was also spoken in Scania and Halland, Sweden. F1 da, da-bornholm BitextMining poetry, wiki, fiction, web, social 1000 44.36 (std: 41.22) DKHate Danish Tweets annotated for Hate Speech either being Offensive or not Accuracy da Classification social 329 88.18 (std: 168.30) Da Political Comments A dataset of Danish political comments rated for sentiment Accuracy da Classification social 7206 69.60 (std: 62.85) DaLAJ A Swedish dataset for linguistic acceptability. Available as a part of Superlim. Accuracy sv Classification fiction, non-fiction 888 120.77 (std: 67.95) DanFEVER A Danish dataset intended for misinformation research. It follows the same format as the English FEVER dataset. Ndcg_at_10 da Retrieval wiki, non-fiction 8897 124.84 (std: 168.53) LCC The leipzig corpora collection, annotated for sentiment Accuracy da Classification legal, web, news, social, fiction, non-fiction, academic, government 150 118.73 (std: 57.82) Language Identification A dataset for Nordic language identification. Accuracy da, sv, nb, nn, is, fo Classification wiki 3000 78.23 (std: 48.54) Massive Intent MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages Accuracy da, nb, sv Classification spoken 15021 34.65 (std: 16.99) Massive Scenario MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages Accuracy da, nb, sv Classification spoken 15021 34.65 (std: 16.99) NoReC A Norwegian dataset for sentiment classification on review Accuracy nb Classification reviews 2048 89.62 (std: 61.21) NorQuad Human-created question for Norwegian wikipedia passages. Ndcg_at_10 nb Retrieval non-fiction, wiki 2602 502.19 (std: 875.23) Norwegian courts Nynorsk and Bokm\u00e5l parallel corpus from Norwegian courts. Norway has two standardised written languages. Bokm\u00e5l is a variant closer to Danish, while Nynorsk was created to resemble regional dialects of Norwegian. F1 nb, nn BitextMining legal, non-fiction 456 82.11 (std: 49.48) Norwegian parliament Norwegian parliament speeches annotated with the party of the speaker (<code>Sosialistisk Venstreparti</code> vs <code>Fremskrittspartiet</code>) Accuracy nb Classification spoken 2400 1897.51 (std: 1988.62) ScaLA A linguistic acceptability task for Danish, Norwegian Bokm\u00e5l Norwegian Nynorsk and Swedish. Accuracy da, nb, sv, nn Classification fiction, news, non-fiction, spoken, blog 8192 102.45 (std: 55.49) SweFAQ A Swedish QA dataset derived from FAQ Ndcg_at_10 sv Retrieval non-fiction, web 1024 195.44 (std: 209.33) SweReC A Swedish dataset for sentiment classification on review Accuracy sv Classification reviews 2048 318.83 (std: 499.57) SwednClustering The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters. V_measure sv Clustering non-fiction, news 2048 1619.71 (std: 2220.36) SwednRetrieval News Article Summary Semantic Similarity Estimation. Ndcg_at_10 sv Retrieval non-fiction, news 3070 1946.35 (std: 3071.98) TV2Nord Retrieval News Article and corresponding summaries extracted from the Danish newspaper TV2 Nord. Ndcg_at_10 da Retrieval news, non-fiction 4096 784.11 (std: 982.97) Twitterhjerne Danish question asked on Twitter with the Hashtag #Twitterhjerne ('Twitter brain') and their corresponding answer. Ndcg_at_10 da Retrieval social 340 138.23 (std: 82.41) VG Clustering Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus. V_measure nb Clustering non-fiction, news 2048 1009.65 (std: 1597.60)"},{"location":"datasets/#dataset-licenses","title":"Dataset Licenses","text":"Dataset License Angry Tweets CC-BY-4.0 Bornholm Parallel CC-BY-4.0 DKHate CC-BY-4.0 Da Political Comments DaLAJ CC-BY-4.0 DanFEVER CC-BY-4.0 LCC CC-BY-4.0 Massive Scenario CC-BY-4.0 NoReC CC-BY-NC-4.0 NorQuad CC0-1.0 Norwegian courts MIT Norwegian parliament CC-BY-4.0 ScaLA CC-BY-SA-4.0 SweFAQ CC-BY-4.0 SweReC CC-BY-4.0 SwednClustering CC-BY-4.0 SwednRetrieval CC-BY-4.0 TV2Nord Retrieval Apache 2.0 Twitterhjerne CC BY 4.0 VG Clustering CC-BY-NC"},{"location":"datasets/#dataset-disclaimer","title":"Dataset Disclaimer","text":"<ul> <li>We do not own or host any of the datasets which we use for this benchmark.</li> <li>We only offer refer to existing dataset that we believe we are free to redistribute. If any doubt occurs about the legality of any of our file downloads we will take them off right away after contacting us.</li> </ul> <p>Notice and take down policy</p> <ul> <li>Notice: Should you consider that data used by the dataset contains material that is owned by you and should therefore not be reproduced here, please: Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.</li> <li>Clearly identify the copyrighted work claimed to be infringed.</li> <li>Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.</li> <li>And contact the 'Scandinavian Embedding Benchmark' at the following ticket service: https://frontoffice.chcaa.au.dk/hc/en-us/requests/new</li> </ul> <p>We will comply to legitimate requests by removing the affected sources from the next release of the benchmark.</p>"},{"location":"domains/","title":"Domains","text":"<p>This section examines coverage and performance across task types in SEB. The domains follows the categories used in the Universal Dependencies project.</p>"},{"location":"domains/#performance-across-domains","title":"Performance across Domains","text":"<p>The table show the performance across domains in the Scandinavian Embedding Benchmark.</p>"},{"location":"domains/#coverage","title":"Coverage","text":"<p>The following table show the coverage pr. language. Note that some are only partially includes. This is due to some text partially including data from the domain though it is not considered the majority.</p> Across Danish Norwegian Bokm\u00e5l Norwegian Nynorsk Swedish Domain Academic (\u2713) (\u2713) Bible Blog Fiction \u2713 \u2713 \u2713 \u2713 \u2713 Government \u2713 \u2713 \u2713 \u2713 \u2713 Legal \u2713 (\u2713) \u2713 \u2713 Medical News \u2713 \u2713 \u2713 \u2713 Non-Fiction \u2713 \u2713 \u2713 \u2713 Poetry ( \u2713 ) (\u2713) Reviews \u2713 \u2713 \u2713 Social \u2713 \u2713 \u2713 Spoken \u2713 \u2713 \u2713 \u2713 Wiki \u2713 \u2713 \u2713 \u2713 \u2713 Web \u2713 \u2713 \u2713"},{"location":"getting_started/","title":"Getting started","text":"In\u00a0[1]: Copied! <pre>%%bash\n\nseb --help\n</pre> %%bash  seb --help <pre>\nAvailable commands:\n\n  run   Runs the Benchmark either on specified models or on all registered mod...\n\n</pre> <p>or for more on the specific command you can call <code>seb {command} --help</code>. To run a model using the CLI you can run it like so:</p> In\u00a0[2]: Copied! <pre>%%bash\nseb run -m all-MiniLM-L6-v2 --output-path model_results/\n</pre> %%bash seb run -m all-MiniLM-L6-v2 --output-path model_results/ <pre>INFO:seb.cli.run:Model registered in SEB. Loading from registry.\nRunning all-MiniLM-L6-v2:   0%|          | 0/1 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on Angry Tweets:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on LCC:   0%|          | 0/16 [00:00&lt;?, ?it/s]         \nRunning all-MiniLM-L6-v2 on Bornholm Parallel:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on DKHate:   0%|          | 0/16 [00:00&lt;?, ?it/s]           \nRunning all-MiniLM-L6-v2 on Da Political Comments:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on Massive Intent:   0%|          | 0/16 [00:00&lt;?, ?it/s]       \nRunning all-MiniLM-L6-v2 on Massive Scenario:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on ScaLA:   0%|          | 0/16 [00:00&lt;?, ?it/s]           \nRunning all-MiniLM-L6-v2 on Language Identification:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on NoReC:   0%|          | 0/16 [00:00&lt;?, ?it/s]                  \nRunning all-MiniLM-L6-v2 on Norwegian parliament:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on VGSummarizationClustering:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on SweReC:   0%|          | 0/16 [00:00&lt;?, ?it/s]                   \nRunning all-MiniLM-L6-v2 on DaLAJ:   0%|          | 0/16 [00:00&lt;?, ?it/s] \nRunning all-MiniLM-L6-v2 on SweFAQ:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2 on SwednClustering:   0%|          | 0/16 [00:00&lt;?, ?it/s]\nRunning all-MiniLM-L6-v2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 25.99it/s]            \nERROR:seb.benchmark:Error when running VGSummarizationClustering on embed-multilingual-v3.0: Cache for embed-multilingual-v3.0 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on embed-multilingual-v3.0: Cache for embed-multilingual-v3.0 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on paraphrase-multilingual-MiniLM-L12-v2: Cache for paraphrase-multilingual-MiniLM-L12-v2 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on paraphrase-multilingual-MiniLM-L12-v2: Cache for paraphrase-multilingual-MiniLM-L12-v2 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on paraphrase-multilingual-mpnet-base-v2: Cache for paraphrase-multilingual-mpnet-base-v2 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on paraphrase-multilingual-mpnet-base-v2: Cache for paraphrase-multilingual-mpnet-base-v2 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on sentence-bert-swedish-cased: Cache for sentence-bert-swedish-cased on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on sentence-bert-swedish-cased: Cache for sentence-bert-swedish-cased on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on electra-small-nordic: Cache for electra-small-nordic on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on electra-small-nordic: Cache for electra-small-nordic on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on DanskBERT: Cache for DanskBERT on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on DanskBERT: Cache for DanskBERT on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-encoder-large-v1: Cache for dfm-encoder-large-v1 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-encoder-large-v1: Cache for dfm-encoder-large-v1 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on nb-bert-large: Cache for nb-bert-large on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on nb-bert-large: Cache for nb-bert-large on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on nb-bert-base: Cache for nb-bert-base on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on nb-bert-base: Cache for nb-bert-base on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on bert-base-swedish-cased: Cache for bert-base-swedish-cased on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on bert-base-swedish-cased: Cache for bert-base-swedish-cased on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on electra-small-swedish-cased-discriminator: Cache for electra-small-swedish-cased-discriminator on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on electra-small-swedish-cased-discriminator: Cache for electra-small-swedish-cased-discriminator on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on xlm-roberta-base: Cache for xlm-roberta-base on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on xlm-roberta-base: Cache for xlm-roberta-base on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-sentence-encoder-large-1: Cache for dfm-sentence-encoder-large-1 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-sentence-encoder-large-1: Cache for dfm-sentence-encoder-large-1 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-sentence-encoder-large-exp1: Cache for dfm-sentence-encoder-large-exp1 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-sentence-encoder-large-exp1: Cache for dfm-sentence-encoder-large-exp1 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-sentence-encoder-small-v1: Cache for dfm-sentence-encoder-small-v1 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-sentence-encoder-small-v1: Cache for dfm-sentence-encoder-small-v1 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-sentence-encoder-medium-v1: Cache for dfm-sentence-encoder-medium-v1 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-sentence-encoder-medium-v1: Cache for dfm-sentence-encoder-medium-v1 on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on dfm-sentence-encoder-large-exp2-no-lang-align: Cache for dfm-sentence-encoder-large-exp2-no-lang-align on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on dfm-sentence-encoder-large-exp2-no-lang-align: Cache for dfm-sentence-encoder-large-exp2-no-lang-align on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on e5-small: Cache for e5-small on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on e5-small: Cache for e5-small on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on e5-base: Cache for e5-base on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on e5-base: Cache for e5-base on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on e5-large: Cache for e5-large on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on e5-large: Cache for e5-large on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on multilingual-e5-base: Cache for multilingual-e5-base on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on multilingual-e5-base: Cache for multilingual-e5-base on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on multilingual-e5-large: Cache for multilingual-e5-large on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on multilingual-e5-large: Cache for multilingual-e5-large on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on e5-mistral-7b-instruct: Cache for e5-mistral-7b-instruct on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on e5-mistral-7b-instruct: Cache for e5-mistral-7b-instruct on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on sonar-dan: Cache for sonar-dan on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on sonar-dan: Cache for sonar-dan on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on sonar-swe: Cache for sonar-swe on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on sonar-swe: Cache for sonar-swe on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on sonar-nob: Cache for sonar-nob on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on sonar-nob: Cache for sonar-nob on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on sonar-nno: Cache for sonar-nno on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on sonar-nno: Cache for sonar-nno on SwednClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running VGSummarizationClustering on text-embedding-ada-002: Cache for text-embedding-ada-002 on VGSummarizationClustering does not exist. Set run_model=True to run the model.\nERROR:seb.benchmark:Error when running SwednClustering on text-embedding-ada-002: Cache for text-embedding-ada-002 on SwednClustering does not exist. Set run_model=True to run the model.\n</pre> <pre>                                      Benchmark Results                         \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2533\u2501\u2501\u2533\u2501\u2533\u2501\u2501\u2533\u2501\u2533\u2501\u2501\u2533\u2501\u2533\u2501\u2501\u2533\u2501\u2533\u2501\u2501\u2533\u2501\n\u2503      \u2503                         \u2503 Average \u2503 Average \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \n\u2503 Rank \u2503 Model                   \u2503   Score \u2503    Rank \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \u2503  \u2503 \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2547\u2501\u2501\u2547\u2501\u2547\u2501\u2501\u2547\u2501\u2547\u2501\u2501\u2547\u2501\u2547\u2501\u2501\u2547\u2501\u2547\u2501\u2501\u2547\u2501\n\u2502    1 \u2502 multilingual-e5-small   \u2502    0.53 \u2502    9.72 \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \n\u2502    2 \u2502 NEW: all-MiniLM-L6-v2   \u2502    0.40 \u2502   22.12 \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \n\u2502    3 \u2502 embed-multilingual-v3.0 \u2502     nan \u2502    5.39 \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \u2502  \u2502 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2534\u2500\u2534\u2500\u2500\u2534\u2500\n</pre> <p>For how to run the benchmark on all models or only on a subset of tasks check out the documentation for the CLI.</p> In\u00a0[3]: Copied! <pre>import seb\n\nmodel = seb.get_model(\"jonfd/electra-small-nordic\")\ntask = seb.get_task(\"DKHate\")\n\n# initialize benchmark with tasks\nbenchmark = seb.Benchmark(tasks=[task])\n\n# benchmark the model\nbenchmark_result = benchmark.evaluate_model(model)\n</pre> import seb  model = seb.get_model(\"jonfd/electra-small-nordic\") task = seb.get_task(\"DKHate\")  # initialize benchmark with tasks benchmark = seb.Benchmark(tasks=[task])  # benchmark the model benchmark_result = benchmark.evaluate_model(model) <pre></pre> In\u00a0[4]: Copied! <pre>benchmark_result  # examine output\n</pre> benchmark_result  # examine output Out[4]: <pre>BenchmarkResults(meta=ModelMeta(name='electra-small-nordic', description=None, huggingface_name='jonfd/electra-small-nordic', reference='https://huggingface.co/jonfd/electra-small-nordic', languages=['da', 'nb', 'sv', 'nn'], open_source=True, embedding_size=256), task_results=[TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 30, 13, 55, 38, 480327), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.8950480900418238, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.013877821318913264, 'main_score': 0.5945288753799393}}, main_score='accuracy')])</pre> In\u00a0[5]: Copied! <pre>benchmark_result[0]  # examine the results for the first task\n</pre> benchmark_result[0]  # examine the results for the first task Out[5]: <pre>TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 30, 13, 55, 38, 480327), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.8950480900418238, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.013877821318913264, 'main_score': 0.5945288753799393}}, main_score='accuracy')</pre> In\u00a0[11]: Copied! <pre>models = [seb.get_model(\"all-MiniLM-L6-v2\")]\n# for simplicity, we will only run it with one model, but you could run it with multiple models:\n# models = seb.get_all_models()\n\nfull_benchmark = seb.Benchmark()\nresults = benchmark.evaluate_models(models=models)\n</pre> models = [seb.get_model(\"all-MiniLM-L6-v2\")] # for simplicity, we will only run it with one model, but you could run it with multiple models: # models = seb.get_all_models()  full_benchmark = seb.Benchmark() results = benchmark.evaluate_models(models=models) <pre>Running all-MiniLM-L6-v2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 175.16it/s]\n</pre> <p>This runs the full benchmark on all the specified models as well as all the registrered datasets. Note that all benchmark results are cached as included as a part of the package, this means that you won't have to rerun results that are already run.</p> In\u00a0[15]: Copied! <pre>mdl_result_on_benchmark = results[0]  # results for the first model\n\nmdl_result_on_benchmark[0]  # results for the first task\n</pre> mdl_result_on_benchmark = results[0]  # results for the first model  mdl_result_on_benchmark[0]  # results for the first task Out[15]: <pre>TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='1.1.0', time_of_run=datetime.datetime(2023, 7, 31, 15, 19, 48, 879189), scores={'da': {'accuracy': 0.5504559270516718, 'f1': 0.4487544754943351, 'ap': 0.8825715897823836, 'accuracy_stderr': 0.08179003177509295, 'f1_stderr': 0.04439449341359171, 'ap_stderr': 0.008146255235874632, 'main_score': 0.5504559270516718}}, main_score='accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom typing import Any\nimport seb\nimport numpy as np\n\n\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n\nclass MyEncoder(seb.Encoder):\n\"\"\"\n    A custom model for SEB that uses the SentenceTransformer library.\n    \"\"\"\n\n    def __init__(self):\n        self.model = SentenceTransformer(model_name)\n\n    def encode(  # type: ignore\n        self,\n        sentences: list[str],\n        *,\n        task: seb.Task,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        if task.name == \"DKHate\":  # allow you to embed differently based on the task\n            emb = self.model.encode(sentences, batch_size=32, **kwargs)\n        else:\n            emb = self.model.encode(sentences, batch_size=32, **kwargs)  # here we just do the same for all tasks\n        return emb\n\n\n@seb.models.register(model_name)  # add the model to the registry\ndef create_my_model() -&gt; seb.SebModel:\n    hf_name = model_name\n\n    # create meta data\n    meta = seb.ModelMeta(\n        name=hf_name.split(\"/\")[-1],\n        huggingface_name=hf_name,\n        reference=\"https://huggingface.co/{hf_name}\",\n        languages=[],\n        embedding_size=384,\n    )\n    return seb.SebModel(\n        encoder=MyEncoder(),\n        meta=meta,\n    )\n</pre> from sentence_transformers import SentenceTransformer from typing import Any import seb import numpy as np   model_name = \"sentence-transformers/all-MiniLM-L6-v2\"   class MyEncoder(seb.Encoder):     \"\"\"     A custom model for SEB that uses the SentenceTransformer library.     \"\"\"      def __init__(self):         self.model = SentenceTransformer(model_name)      def encode(  # type: ignore         self,         sentences: list[str],         *,         task: seb.Task,         **kwargs: Any,     ) -&gt; np.ndarray:         if task.name == \"DKHate\":  # allow you to embed differently based on the task             emb = self.model.encode(sentences, batch_size=32, **kwargs)         else:             emb = self.model.encode(sentences, batch_size=32, **kwargs)  # here we just do the same for all tasks         return emb   @seb.models.register(model_name)  # add the model to the registry def create_my_model() -&gt; seb.SebModel:     hf_name = model_name      # create meta data     meta = seb.ModelMeta(         name=hf_name.split(\"/\")[-1],         huggingface_name=hf_name,         reference=\"https://huggingface.co/{hf_name}\",         languages=[],         embedding_size=384,     )     return seb.SebModel(         encoder=MyEncoder(),         meta=meta,     ) <p>Note that if you want to use the CLI with one of your own added models you can import registrered functions from a file specified using the <code>--code</code> flag.</p>"},{"location":"getting_started/#getting-started","title":"Getting started\u00b6","text":"<p>This is a minimal guide on how to get started using SEB. If you feel like the documentation is lacking feel free to file an issue.</p>"},{"location":"getting_started/#using-the-cli","title":"Using the CLI\u00b6","text":"<p>SEB comes with a simple cli to allow you to run models. This section will show a minimal example of how to use the CLI but if you want to know more check out the CLI documentation. To get a list of available commands you can simply run:</p>"},{"location":"getting_started/#running-a-task","title":"Running a task\u00b6","text":"<p>To run a task you will need to fetch the task amd a model run it.</p>"},{"location":"getting_started/#reproducing-the-benchmark","title":"Reproducing the Benchmark\u00b6","text":"<p>Reproducing the benchmark is easy and is doable simply using the following command:</p>"},{"location":"getting_started/#adding-a-model","title":"Adding a model\u00b6","text":"<p>The benchmark uses a registry to add models. A model in <code>seb</code> includes two thing. 1) a metadata object (<code>seb.ModelMeta</code>) describing the metadata of the model and 2) a loader for the model itself, which is an object that needs an encode methods as described by the <code>seb.ModelInterface</code>. Here is a minimal example of how to add a new model:</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install the <code>seb</code> via pip from PyPI:</p> <pre><code>pip install seb\n</code></pre> <p>or from GitHub using:</p> <pre><code>pip install git+https://github.com/KennethEnevoldsen/scandinavian-embedding-benchmark\n</code></pre>"},{"location":"paper/","title":"Introduction","text":"<ul> <li>importance of embeddings models (search, RAG)</li> <li>few benchmarks for scandinavian languages</li> </ul>"},{"location":"paper/#contributions","title":"Contributions:","text":"<ul> <li>Creates benchmark for scandinavian languages</li> <li>integrates with MTEB</li> <li>with broad coverage of both domains and use-cases</li> <li>Allow for custom encoding methods dependent on task (as opposed to mteb)</li> <li>Added a series of new datasets (?)</li> <li>easily extendable</li> </ul>"},{"location":"paper/#design-principles","title":"(Design principles)","text":"<ul> <li>flexible (easy to add new models)</li> <li>easy to run on even small laptops</li> <li>minimal dependencies besides MTEB</li> <li>It should be transparent how models are run as often the exact prompt used can notably influence performance. --&gt; this models are implemented as a part of the bencmark.</li> </ul>"},{"location":"paper/#results","title":"Results","text":"<ul> <li> <p>conflict between language identification and lang. alignment</p> </li> <li> <p>translation then embed comparison</p> </li> </ul>"},{"location":"speed_performance/","title":"Speed and Performance","text":"<p>This following figure examines relation between speed and performance on the Scandinavian Embedding Benchmark. API models are not included as speeds may vary.</p>"},{"location":"task_type/","title":"Task Types","text":"<p>This section examines coverage and performance across task types in SEB. The task types categories are derived from the MTEB benchmark.</p>"},{"location":"task_type/#performance-across-tasks-types","title":"Performance across Tasks Types","text":"<p>The table show the performance across task types:</p>"},{"location":"task_type/#coverage","title":"Coverage","text":"<p>The follows table give you and an overview of the coverage of the tasks: </p> Across \u00b7           Danish Norwegian Bokm\u00e5l Norwegian Nynorsk Swedish Formalization Task Retrieval Question answering \u2713 \u2713 \u2713 \u2713 article retrieval \u2713 \u2713 \u2713 \u2713 bitext Mining dialect pairing \u2713 \u2713 \u2713 \u2713 Classification Political \u2713 \u2713 \u2713 Language Identification \u2713 \u2713 \u2713 \u2713 \u2713 Linguistic Acceptability \u2713 \u2713 \u2713 \u2713 \u2713 Sentiment/Hate Speech \u2713 \u2713 \u2713 \u2713 Dialog Systems \u2713 \u2713 \u2713 \u2713 \u2713 Clustering Thematic Clustering \u2713 \u2713 \u2713 Reranking Pair Classification STS"},{"location":"update_benchmark_tables/","title":"Update benchmark tables","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Script for running the benchmark and pushing the results to Datawrapper.\n\nExample:\n    python update_benchmark_tables.py --data-wrapper-api-token &lt;token&gt;\n\"\"\"\n</pre> \"\"\"Script for running the benchmark and pushing the results to Datawrapper.  Example:     python update_benchmark_tables.py --data-wrapper-api-token  \"\"\" In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom typing import Optional\n</pre> import argparse from collections import defaultdict from collections.abc import Sequence from typing import Optional In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom datawrapper import Datawrapper\n</pre> import numpy as np import pandas as pd from datawrapper import Datawrapper In\u00a0[\u00a0]: Copied! <pre>import seb\nfrom seb.full_benchmark import BENCHMARKS\nfrom seb.registered_tasks.speed import CPUSpeedTask\n</pre> import seb from seb.full_benchmark import BENCHMARKS from seb.registered_tasks.speed import CPUSpeedTask In\u00a0[\u00a0]: Copied! <pre>subset_to_chart_id = {\n    \"Mainland Scandinavian\": \"7Nwjx\",\n    \"Danish\": \"us1YK\",\n    \"Norwegian\": \"pV87q\",\n    \"Swedish\": \"aL23t\",\n    \"Domain\": \"F00q5\",\n    \"Task Type\": \"4jkip\",\n    \"Speed x Performance\": \"oXdUJ\",\n}\n</pre> subset_to_chart_id = {     \"Mainland Scandinavian\": \"7Nwjx\",     \"Danish\": \"us1YK\",     \"Norwegian\": \"pV87q\",     \"Swedish\": \"aL23t\",     \"Domain\": \"F00q5\",     \"Task Type\": \"4jkip\",     \"Speed x Performance\": \"oXdUJ\", } In\u00a0[\u00a0]: Copied! <pre>datawrapper_lang_codes = {\n    \"da\": \"dk\",\n    \"nb\": \"no\",\n    \"nn\": \"no\",\n    \"sv\": \"se\",\n    \"en\": \"us\",\n}\n</pre> datawrapper_lang_codes = {     \"da\": \"dk\",     \"nb\": \"no\",     \"nn\": \"no\",     \"sv\": \"se\",     \"en\": \"us\", } In\u00a0[\u00a0]: Copied! <pre>def get_main_score(task: seb.TaskResult, langs: list[str]) -&gt; float:\n    _langs = set(langs) &amp; set(task.languages)\n    return task.get_main_score(_langs) * 100\n</pre> def get_main_score(task: seb.TaskResult, langs: list[str]) -&gt; float:     _langs = set(langs) &amp; set(task.languages)     return task.get_main_score(_langs) * 100 In\u00a0[\u00a0]: Copied! <pre>def get_flag(languages: Sequence[str]) -&gt; str:\n    if languages:\n        flags = []\n        for l in languages:\n            if l in datawrapper_lang_codes:\n                flags.append(datawrapper_lang_codes[l])\n        flags = list(set(flags))  # remove duplicates (e.g. nb and nn)\n        return \" \".join([f\":{f}:\" for f in flags])\n    return \"\ud83c\udf10\"\n</pre> def get_flag(languages: Sequence[str]) -&gt; str:     if languages:         flags = []         for l in languages:             if l in datawrapper_lang_codes:                 flags.append(datawrapper_lang_codes[l])         flags = list(set(flags))  # remove duplicates (e.g. nb and nn)         return \" \".join([f\":{f}:\" for f in flags])     return \"\ud83c\udf10\" In\u00a0[\u00a0]: Copied! <pre>def open_source_to_string(open_source: bool) -&gt; str:\n    return \"\u2713\" if open_source else \"\u2717\"\n</pre> def open_source_to_string(open_source: bool) -&gt; str:     return \"\u2713\" if open_source else \"\u2717\" In\u00a0[\u00a0]: Copied! <pre>def create_mdl_name_w_reference(mdl: seb.ModelMeta) -&gt; str:\n    reference = mdl.reference\n    name: str = mdl.name\n\n    mdl_name = f\"[{name}]({reference})\" if reference else name\n    lang_flag = get_flag(mdl.languages)\n    mdl_name = f\"{mdl_name} {lang_flag}\"\n\n    return mdl_name\n</pre> def create_mdl_name_w_reference(mdl: seb.ModelMeta) -&gt; str:     reference = mdl.reference     name: str = mdl.name      mdl_name = f\"[{name}]({reference})\" if reference else name     lang_flag = get_flag(mdl.languages)     mdl_name = f\"{mdl_name} {lang_flag}\"      return mdl_name In\u00a0[\u00a0]: Copied! <pre>def get_speed_results(model_meta: seb.ModelMeta) -&gt; float | None:\n    model = seb.get_model(model_meta.name)\n    TOKENS_IN_UGLY_DUCKLING = 3591\n\n    speed_task = CPUSpeedTask()\n    speed_result = seb.run_task(speed_task, model, raise_errors=False, use_cache=True, run_model=False)\n    if isinstance(speed_result, seb.TaskResult):\n        speed_in_seconds = speed_result.get_main_score()\n        word_per_seconds = TOKENS_IN_UGLY_DUCKLING / speed_in_seconds\n        return word_per_seconds\n    return None\n</pre> def get_speed_results(model_meta: seb.ModelMeta) -&gt; float | None:     model = seb.get_model(model_meta.name)     TOKENS_IN_UGLY_DUCKLING = 3591      speed_task = CPUSpeedTask()     speed_result = seb.run_task(speed_task, model, raise_errors=False, use_cache=True, run_model=False)     if isinstance(speed_result, seb.TaskResult):         speed_in_seconds = speed_result.get_main_score()         word_per_seconds = TOKENS_IN_UGLY_DUCKLING / speed_in_seconds         return word_per_seconds     return None In\u00a0[\u00a0]: Copied! <pre>def benchmark_result_to_row(\n    result: seb.BenchmarkResults,\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    mdl_name_w_link = create_mdl_name_w_reference(result.meta)\n    # sort by task name\n    task_results = result.task_results\n    sorted_tasks = sorted(task_results, key=lambda t: t.task_name)\n    task_names = [t.task_name for t in sorted_tasks]\n    scores = [get_main_score(t, langs) for t in sorted_tasks]  # type: ignore\n\n    df = pd.DataFrame([scores], columns=task_names, index=[mdl_name_w_link])\n    df[\"Model name\"] = result.meta.name\n    df[\"Average Score\"] = result.get_main_score() * 100\n    df[\"Open Source\"] = open_source_to_string(result.meta.open_source)\n    df[\"Embedding Size\"] = result.meta.embedding_size\n    df[\"WPS (CPU)\"] = get_speed_results(result.meta)\n    return df\n</pre> def benchmark_result_to_row(     result: seb.BenchmarkResults,     langs: list[str], ) -&gt; pd.DataFrame:     mdl_name_w_link = create_mdl_name_w_reference(result.meta)     # sort by task name     task_results = result.task_results     sorted_tasks = sorted(task_results, key=lambda t: t.task_name)     task_names = [t.task_name for t in sorted_tasks]     scores = [get_main_score(t, langs) for t in sorted_tasks]  # type: ignore      df = pd.DataFrame([scores], columns=task_names, index=[mdl_name_w_link])     df[\"Model name\"] = result.meta.name     df[\"Average Score\"] = result.get_main_score() * 100     df[\"Open Source\"] = open_source_to_string(result.meta.open_source)     df[\"Embedding Size\"] = result.meta.embedding_size     df[\"WPS (CPU)\"] = get_speed_results(result.meta)     return df In\u00a0[\u00a0]: Copied! <pre>def create_n_datasets_row_for_domains() -&gt; pd.DataFrame:\n    tasks: list[seb.Task] = seb.get_all_tasks()\n    domains = sorted({d for t in tasks for d in t.domain})\n    domain2tasks = {d: [t.name for t in tasks if d in t.domain] for d in domains}\n    scores = []\n    domain_names = []\n    n_datasets = []\n    for d, ts in domain2tasks.items():\n        domain_names.append(d.capitalize())\n        n_datasets.append(len(ts))\n    return pd.DataFrame([n_datasets], columns=domain_names, index=[\"N. Datasets\"])\n</pre> def create_n_datasets_row_for_domains() -&gt; pd.DataFrame:     tasks: list[seb.Task] = seb.get_all_tasks()     domains = sorted({d for t in tasks for d in t.domain})     domain2tasks = {d: [t.name for t in tasks if d in t.domain] for d in domains}     scores = []     domain_names = []     n_datasets = []     for d, ts in domain2tasks.items():         domain_names.append(d.capitalize())         n_datasets.append(len(ts))     return pd.DataFrame([n_datasets], columns=domain_names, index=[\"N. Datasets\"]) In\u00a0[\u00a0]: Copied! <pre>def create_n_datasets_row_for_task_types() -&gt; pd.DataFrame:\n    tasks: list[seb.Task] = seb.get_all_tasks()\n    task_type = sorted({t_type for t in tasks for t_type in [t.task_type, *t.task_subtypes]})\n    tasktype2tasks = {tt: [t.name for t in tasks if tt == t.task_type or tt in t.task_subtypes] for tt in task_type}\n    scores = []\n    task_type_names = []\n    n_datasets = []\n    for t, ts in tasktype2tasks.items():\n        task_type_names.append(t.capitalize())\n        n_datasets.append(len(ts))\n    return pd.DataFrame([n_datasets], columns=task_type_names, index=[\"N. Datasets\"])\n</pre> def create_n_datasets_row_for_task_types() -&gt; pd.DataFrame:     tasks: list[seb.Task] = seb.get_all_tasks()     task_type = sorted({t_type for t in tasks for t_type in [t.task_type, *t.task_subtypes]})     tasktype2tasks = {tt: [t.name for t in tasks if tt == t.task_type or tt in t.task_subtypes] for tt in task_type}     scores = []     task_type_names = []     n_datasets = []     for t, ts in tasktype2tasks.items():         task_type_names.append(t.capitalize())         n_datasets.append(len(ts))     return pd.DataFrame([n_datasets], columns=task_type_names, index=[\"N. Datasets\"]) In\u00a0[\u00a0]: Copied! <pre>def benchmark_result_to_domain_row(\n    result: seb.BenchmarkResults,\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    tasks: list[seb.Task] = seb.get_all_tasks()\n    domains = sorted({d for t in tasks for d in t.domain})\n    domain2tasks = {d: [t.name for t in tasks if d in t.domain] for d in domains}\n\n    scores = []\n    domain_names = []\n    n_datasets = []\n    for d, ts in domain2tasks.items():\n        task_results = [r for r in result.task_results if r.task_name in ts]\n        _scores = np.array([get_main_score(t, langs) for t in task_results])  # type: ignore\n        score = np.mean(_scores)\n        scores.append(score)\n        domain_names.append(d.capitalize())\n        n_datasets.append(len(ts))\n\n    mdl_name = create_mdl_name_w_reference(result.meta)\n    df = pd.DataFrame([scores], columns=domain_names, index=[mdl_name])\n    df[\"Average Score\"] = result.get_main_score() * 100\n    df[\"Open Source\"] = open_source_to_string(result.meta.open_source)\n    df[\"Embedding Size\"] = result.meta.embedding_size\n    df[\"WPS (CPU)\"] = get_speed_results(result.meta)\n    return df\n</pre> def benchmark_result_to_domain_row(     result: seb.BenchmarkResults,     langs: list[str], ) -&gt; pd.DataFrame:     tasks: list[seb.Task] = seb.get_all_tasks()     domains = sorted({d for t in tasks for d in t.domain})     domain2tasks = {d: [t.name for t in tasks if d in t.domain] for d in domains}      scores = []     domain_names = []     n_datasets = []     for d, ts in domain2tasks.items():         task_results = [r for r in result.task_results if r.task_name in ts]         _scores = np.array([get_main_score(t, langs) for t in task_results])  # type: ignore         score = np.mean(_scores)         scores.append(score)         domain_names.append(d.capitalize())         n_datasets.append(len(ts))      mdl_name = create_mdl_name_w_reference(result.meta)     df = pd.DataFrame([scores], columns=domain_names, index=[mdl_name])     df[\"Average Score\"] = result.get_main_score() * 100     df[\"Open Source\"] = open_source_to_string(result.meta.open_source)     df[\"Embedding Size\"] = result.meta.embedding_size     df[\"WPS (CPU)\"] = get_speed_results(result.meta)     return df In\u00a0[\u00a0]: Copied! <pre>def benchmark_result_to_task_type_row(\n    result: seb.BenchmarkResults,\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    tasks: list[seb.Task] = seb.get_all_tasks()\n    task_type = sorted({t_type for t in tasks for t_type in [t.task_type, *t.task_subtypes]})\n    tasktype2tasks = {tt: [t.name for t in tasks if tt == t.task_type or tt in t.task_subtypes] for tt in task_type}\n\n    scores = []\n    task_type_names = []\n    n_datasets = []\n    for t, ts in tasktype2tasks.items():\n        task_results = [r for r in result.task_results if r.task_name in ts]\n        _scores = np.array([get_main_score(t, langs) for t in task_results])  # type: ignore\n        score = np.mean(_scores)\n        scores.append(score)\n        task_type_names.append(t.capitalize())\n        n_datasets.append(len(ts))\n\n    mdl_name = create_mdl_name_w_reference(result.meta)\n    df = pd.DataFrame([scores], columns=task_type_names, index=[mdl_name])\n    df[\"Average Score\"] = result.get_main_score() * 100\n    df[\"Open Source\"] = open_source_to_string(result.meta.open_source)\n    df[\"Embedding Size\"] = result.meta.embedding_size\n    df[\"WPS (CPU)\"] = get_speed_results(result.meta)\n    return df\n</pre> def benchmark_result_to_task_type_row(     result: seb.BenchmarkResults,     langs: list[str], ) -&gt; pd.DataFrame:     tasks: list[seb.Task] = seb.get_all_tasks()     task_type = sorted({t_type for t in tasks for t_type in [t.task_type, *t.task_subtypes]})     tasktype2tasks = {tt: [t.name for t in tasks if tt == t.task_type or tt in t.task_subtypes] for tt in task_type}      scores = []     task_type_names = []     n_datasets = []     for t, ts in tasktype2tasks.items():         task_results = [r for r in result.task_results if r.task_name in ts]         _scores = np.array([get_main_score(t, langs) for t in task_results])  # type: ignore         score = np.mean(_scores)         scores.append(score)         task_type_names.append(t.capitalize())         n_datasets.append(len(ts))      mdl_name = create_mdl_name_w_reference(result.meta)     df = pd.DataFrame([scores], columns=task_type_names, index=[mdl_name])     df[\"Average Score\"] = result.get_main_score() * 100     df[\"Open Source\"] = open_source_to_string(result.meta.open_source)     df[\"Embedding Size\"] = result.meta.embedding_size     df[\"WPS (CPU)\"] = get_speed_results(result.meta)     return df In\u00a0[\u00a0]: Copied! <pre>def convert_to_table(\n    results: list[seb.BenchmarkResults],\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    rows = [benchmark_result_to_row(result, langs) for result in results]\n    df = pd.concat(rows)\n    df = df.sort_values(by=\"Average Score\", ascending=False)\n    df[\"Average Rank\"] = compute_avg_rank(df)\n    # df[\"Average Rank (Bootstrapped)\"] = compute_avg_rank_bootstrap(df) # noqa\n\n    # ensure that the average and open source are the first column\n    cols = df.columns.tolist()\n    first_columns = [\"Average Score\", \"Average Rank\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]\n    other_cols = sorted(c for c in cols if c not in first_columns)\n    df = df[first_columns + other_cols]\n\n    # convert name to column\n    df = df.reset_index()\n    df = df.rename(columns={\"index\": \"Model\"})\n    df = df.sort_values(by=\"Model\", ascending=True)\n\n    return df\n</pre> def convert_to_table(     results: list[seb.BenchmarkResults],     langs: list[str], ) -&gt; pd.DataFrame:     rows = [benchmark_result_to_row(result, langs) for result in results]     df = pd.concat(rows)     df = df.sort_values(by=\"Average Score\", ascending=False)     df[\"Average Rank\"] = compute_avg_rank(df)     # df[\"Average Rank (Bootstrapped)\"] = compute_avg_rank_bootstrap(df) # noqa      # ensure that the average and open source are the first column     cols = df.columns.tolist()     first_columns = [\"Average Score\", \"Average Rank\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]     other_cols = sorted(c for c in cols if c not in first_columns)     df = df[first_columns + other_cols]      # convert name to column     df = df.reset_index()     df = df.rename(columns={\"index\": \"Model\"})     df = df.sort_values(by=\"Model\", ascending=True)      return df In\u00a0[\u00a0]: Copied! <pre>def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):\n    dw = Datawrapper(access_token=token)\n    assert dw.account_info(), \"Could not connect to Datawrapper\"\n    resp = dw.add_data(chart_id, data=df)\n    assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"\n    iframe_html = dw.publish_chart(chart_id)\n    assert iframe_html, \"Could not publish chart\"\n</pre> def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):     dw = Datawrapper(access_token=token)     assert dw.account_info(), \"Could not connect to Datawrapper\"     resp = dw.add_data(chart_id, data=df)     assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"     iframe_html = dw.publish_chart(chart_id)     assert iframe_html, \"Could not publish chart\" In\u00a0[\u00a0]: Copied! <pre>def compute_avg_rank(df: pd.DataFrame) -&gt; pd.Series:\n\"\"\"For each model in the dataset, for each task, compute the rank of the model and then compute the average rank.\"\"\"\n    df = df.drop(columns=[\"Average Score\", \"Open Source\", \"Embedding Size\", \"Model name\", \"WPS (CPU)\"])\n\n    ranks = df.rank(axis=0, ascending=False, na_option=\"bottom\")\n    avg_ranks = ranks.mean(axis=1)\n    return avg_ranks\n</pre> def compute_avg_rank(df: pd.DataFrame) -&gt; pd.Series:     \"\"\"For each model in the dataset, for each task, compute the rank of the model and then compute the average rank.\"\"\"     df = df.drop(columns=[\"Average Score\", \"Open Source\", \"Embedding Size\", \"Model name\", \"WPS (CPU)\"])      ranks = df.rank(axis=0, ascending=False, na_option=\"bottom\")     avg_ranks = ranks.mean(axis=1)     return avg_ranks In\u00a0[\u00a0]: Copied! <pre>def compute_avg_rank_bootstrap(df: pd.DataFrame, n_samples: int = 100) -&gt; pd.Series:\n\"\"\"For all models bootstrap a set of tasks and compute the average rank. Repeat this n_samples times.\"\"\"\n    df = df.drop(columns=[\"Average Score\", \"Open Source\", \"Embedding Size\", \"Average Rank\", \"WPS (CPU)\", \"Model name\"])\n    tasks = np.array(df.columns.tolist())\n    n_tasks = len(tasks)\n    model2rank = defaultdict(list)\n\n    for _ in range(n_samples):\n        bootstrap_tasks = np.random.choice(tasks, n_tasks, replace=True)\n        ranks = df[bootstrap_tasks].rank(axis=0, ascending=False, na_option=\"bottom\")\n        avg_ranks = ranks.mean(axis=1)\n        for model, rank in avg_ranks.items():\n            model2rank[model].append(rank)\n\n    avg_ranks = {model: np.mean(ranks) for model, ranks in model2rank.items()}\n    ci = {model: np.percentile(ranks, [2.5, 97.5]) for model, ranks in model2rank.items()}\n    # create \"{avg_rank} ({ci_low}-{ci_high})\" string\n    avg_ranks_ = {model: f\"{avg_ranks[model]:.1f} [{ci_low:.1f}, {ci_high:.1f}]\" for model, (ci_low, ci_high) in ci.items()}\n    return pd.Series(avg_ranks_)\n</pre> def compute_avg_rank_bootstrap(df: pd.DataFrame, n_samples: int = 100) -&gt; pd.Series:     \"\"\"For all models bootstrap a set of tasks and compute the average rank. Repeat this n_samples times.\"\"\"     df = df.drop(columns=[\"Average Score\", \"Open Source\", \"Embedding Size\", \"Average Rank\", \"WPS (CPU)\", \"Model name\"])     tasks = np.array(df.columns.tolist())     n_tasks = len(tasks)     model2rank = defaultdict(list)      for _ in range(n_samples):         bootstrap_tasks = np.random.choice(tasks, n_tasks, replace=True)         ranks = df[bootstrap_tasks].rank(axis=0, ascending=False, na_option=\"bottom\")         avg_ranks = ranks.mean(axis=1)         for model, rank in avg_ranks.items():             model2rank[model].append(rank)      avg_ranks = {model: np.mean(ranks) for model, ranks in model2rank.items()}     ci = {model: np.percentile(ranks, [2.5, 97.5]) for model, ranks in model2rank.items()}     # create \"{avg_rank} ({ci_low}-{ci_high})\" string     avg_ranks_ = {model: f\"{avg_ranks[model]:.1f} [{ci_low:.1f}, {ci_high:.1f}]\" for model, (ci_low, ci_high) in ci.items()}     return pd.Series(avg_ranks_) In\u00a0[\u00a0]: Copied! <pre>def create_domain_table(\n    results: list[seb.BenchmarkResults],\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    rows = [benchmark_result_to_domain_row(result, langs) for result in results]\n    df = pd.concat(rows)\n    df = pd.concat([df, create_n_datasets_row_for_domains()])\n    df = df.sort_values(by=\"Average Score\", ascending=False)\n    cols = df.columns.tolist()\n    first_columns = [\"Average Score\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]\n    other_cols = sorted(c for c in cols if c not in first_columns)\n    df = df[first_columns + other_cols]\n\n    # convert name to column\n    df = df.reset_index()\n    df = df.rename(columns={\"index\": \"Model\"})\n    df = df.sort_values(by=\"Model\", ascending=True)\n    return df\n</pre> def create_domain_table(     results: list[seb.BenchmarkResults],     langs: list[str], ) -&gt; pd.DataFrame:     rows = [benchmark_result_to_domain_row(result, langs) for result in results]     df = pd.concat(rows)     df = pd.concat([df, create_n_datasets_row_for_domains()])     df = df.sort_values(by=\"Average Score\", ascending=False)     cols = df.columns.tolist()     first_columns = [\"Average Score\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]     other_cols = sorted(c for c in cols if c not in first_columns)     df = df[first_columns + other_cols]      # convert name to column     df = df.reset_index()     df = df.rename(columns={\"index\": \"Model\"})     df = df.sort_values(by=\"Model\", ascending=True)     return df In\u00a0[\u00a0]: Copied! <pre>def create_task_type_table(\n    results: list[seb.BenchmarkResults],\n    langs: list[str],\n) -&gt; pd.DataFrame:\n    rows = [benchmark_result_to_task_type_row(result, langs) for result in results]\n    df = pd.concat(rows)\n    df = pd.concat([df, create_n_datasets_row_for_task_types()])\n    df = df.sort_values(by=\"Average Score\", ascending=False)\n    cols = df.columns.tolist()\n    first_columns = [\"Average Score\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]\n    other_cols = sorted(c for c in cols if c not in first_columns)\n    df = df[first_columns + other_cols]\n\n    # convert name to column\n    df = df.reset_index()\n    df = df.rename(columns={\"index\": \"Model\"})\n    df = df.sort_values(by=\"Model\", ascending=True)\n    return df\n</pre> def create_task_type_table(     results: list[seb.BenchmarkResults],     langs: list[str], ) -&gt; pd.DataFrame:     rows = [benchmark_result_to_task_type_row(result, langs) for result in results]     df = pd.concat(rows)     df = pd.concat([df, create_n_datasets_row_for_task_types()])     df = df.sort_values(by=\"Average Score\", ascending=False)     cols = df.columns.tolist()     first_columns = [\"Average Score\", \"Open Source\", \"Embedding Size\", \"WPS (CPU)\"]     other_cols = sorted(c for c in cols if c not in first_columns)     df = df[first_columns + other_cols]      # convert name to column     df = df.reset_index()     df = df.rename(columns={\"index\": \"Model\"})     df = df.sort_values(by=\"Model\", ascending=True)     return df In\u00a0[\u00a0]: Copied! <pre>def main(data_wrapper_api_token: str):\n    results = seb.run_benchmark(use_cache=True, run_models=False, raise_errors=False)\n\n    for subset, result in results.items():\n        langs = BENCHMARKS[subset]\n\n        raw_table = convert_to_table(result, langs)\n        table = raw_table.drop(columns=[\"Model name\"])\n        chart_id = subset_to_chart_id[subset]\n        push_to_datawrapper(table, chart_id, data_wrapper_api_token)\n\n        if subset == \"Mainland Scandinavian\":\n            # Update the chart for speed x performance\n            chart_id = subset_to_chart_id[\"Speed x Performance\"]\n            _table = raw_table.drop(columns=[\"Model\"]).rename(columns={\"Model name\": \"Model\"})\n            push_to_datawrapper(_table, chart_id, data_wrapper_api_token)\n\n            # also create the summary charts for task types and domains\n            table = create_domain_table(result, langs)\n            chart_id = subset_to_chart_id[\"Domain\"]\n            push_to_datawrapper(table, chart_id, data_wrapper_api_token)\n\n            table = create_task_type_table(result, langs)\n            chart_id = subset_to_chart_id[\"Task Type\"]\n            push_to_datawrapper(table, chart_id, data_wrapper_api_token)\n</pre> def main(data_wrapper_api_token: str):     results = seb.run_benchmark(use_cache=True, run_models=False, raise_errors=False)      for subset, result in results.items():         langs = BENCHMARKS[subset]          raw_table = convert_to_table(result, langs)         table = raw_table.drop(columns=[\"Model name\"])         chart_id = subset_to_chart_id[subset]         push_to_datawrapper(table, chart_id, data_wrapper_api_token)          if subset == \"Mainland Scandinavian\":             # Update the chart for speed x performance             chart_id = subset_to_chart_id[\"Speed x Performance\"]             _table = raw_table.drop(columns=[\"Model\"]).rename(columns={\"Model name\": \"Model\"})             push_to_datawrapper(_table, chart_id, data_wrapper_api_token)              # also create the summary charts for task types and domains             table = create_domain_table(result, langs)             chart_id = subset_to_chart_id[\"Domain\"]             push_to_datawrapper(table, chart_id, data_wrapper_api_token)              table = create_task_type_table(result, langs)             chart_id = subset_to_chart_id[\"Task Type\"]             push_to_datawrapper(table, chart_id, data_wrapper_api_token) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data-wrapper-api-token\",\n        type=str,\n        required=True,\n        help=\"Datawrapper API token\",\n    )\n\n    args = parser.parse_args()\n    main(args.data_wrapper_api_token)\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser()     parser.add_argument(         \"--data-wrapper-api-token\",         type=str,         required=True,         help=\"Datawrapper API token\",     )      args = parser.parse_args()     main(args.data_wrapper_api_token)"}]}